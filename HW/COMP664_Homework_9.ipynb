{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovi3rNd9BeZ4"
      },
      "source": [
        "# Homework 9\n",
        "\n",
        "In this homework, you will be using a form of attention called *attention pooling* to solve the \"addition problem\". The addition problem was introduced in the [LSTM paper](https://www.bioinf.jku.at/publications/older/2604.pdf) as a way to test whether an RNN could propagate information across many time steps. In the addition problem, the model is given a sequence of 2D vectors in the format:\n",
        "\n",
        "|     |      |     |     |      |     |      |     |     |     |     |\n",
        "|-----|------|-----|-----|------|-----|------|-----|-----|-----|-----|\n",
        "| 0.5 | -0.7 | 0.3 | 0.1 | -0.2 | ... | -0.5 | 0.9 | ... | 0.8 | 0.2 |\n",
        "| 0   |   0  |  1  |  0  |   0  |     |   0  |  1  |     |  0  |  0  |\n",
        "\n",
        "The first dimension of each vector in the sequence is a random number between 0 and 1. The second dimension is 0 for all entries of the sequence expect for 2, where it is 1. The goal of the addition problem is to output the sum of the values in the first dimension at the two indices where the second dimension is 1. In the example above, the target would be 0.9 + 0.3 = 1.2. Below is a code snippet that generates a sequence and its target for the addition problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r42nn-jOxhKp"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def addition_problem(sequence_length=50):\n",
        "    output = np.random.uniform(-1, 1, (sequence_length, 2))\n",
        "    output[:, 0] = 0.\n",
        "    random_indices = np.random.choice(sequence_length, size=2, replace=False)\n",
        "    output[random_indices, [0, 0]] = 1\n",
        "    return output, (output[:, 0]*output[:, 1]).sum(keepdims=True)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addition_problem()"
      ],
      "metadata": {
        "id": "ehrkmhIxAEv6",
        "outputId": "76a4b264-a0e8-4d39-cd67-73e7fe60fa5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.        ,  0.90787634],\n",
              "        [ 0.        , -0.72392442],\n",
              "        [ 0.        ,  0.45645897],\n",
              "        [ 0.        ,  0.65001662],\n",
              "        [ 0.        , -0.42878328],\n",
              "        [ 0.        ,  0.50319949],\n",
              "        [ 0.        , -0.81027942],\n",
              "        [ 0.        , -0.57592772],\n",
              "        [ 0.        ,  0.77575195],\n",
              "        [ 0.        ,  0.21064778],\n",
              "        [ 0.        ,  0.61380194],\n",
              "        [ 0.        , -0.49043025],\n",
              "        [ 0.        ,  0.52713338],\n",
              "        [ 0.        , -0.28426923],\n",
              "        [ 0.        , -0.76214322],\n",
              "        [ 0.        ,  0.56527765],\n",
              "        [ 0.        , -0.36905992],\n",
              "        [ 0.        , -0.35552643],\n",
              "        [ 0.        ,  0.83273812],\n",
              "        [ 0.        ,  0.85160927],\n",
              "        [ 1.        , -0.75418596],\n",
              "        [ 0.        ,  0.1134291 ],\n",
              "        [ 0.        , -0.20819938],\n",
              "        [ 0.        , -0.27355617],\n",
              "        [ 0.        ,  0.07258713],\n",
              "        [ 0.        , -0.622459  ],\n",
              "        [ 0.        ,  0.54978893],\n",
              "        [ 0.        , -0.1757747 ],\n",
              "        [ 0.        , -0.17813834],\n",
              "        [ 0.        , -0.18903376],\n",
              "        [ 0.        , -0.87561835],\n",
              "        [ 0.        ,  0.00579342],\n",
              "        [ 0.        , -0.5737416 ],\n",
              "        [ 0.        , -0.84868908],\n",
              "        [ 0.        , -0.95729745],\n",
              "        [ 0.        , -0.61409494],\n",
              "        [ 0.        ,  0.04063535],\n",
              "        [ 0.        , -0.01849709],\n",
              "        [ 0.        , -0.77914651],\n",
              "        [ 0.        ,  0.07591335],\n",
              "        [ 0.        ,  0.15942379],\n",
              "        [ 0.        ,  0.08418328],\n",
              "        [ 0.        , -0.90035192],\n",
              "        [ 0.        ,  0.2111568 ],\n",
              "        [ 0.        , -0.92359423],\n",
              "        [ 0.        ,  0.1413989 ],\n",
              "        [ 0.        ,  0.72303194],\n",
              "        [ 0.        ,  0.85558649],\n",
              "        [ 1.        , -0.62336612],\n",
              "        [ 0.        , -0.21589744]]), array([-1.37755208]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew0FypwYCwpn"
      },
      "source": [
        "Attention pooling is a form of attention that allows a model to solve the addition problem without using an RNN. In attention pooling, the query vector $q$ is a *learnable parameter*. The keys and values are both the input sequence. Specifically, given a sequence $\\{h_1, h_2, \\ldots, h_T\\}$, attention pooling computes\n",
        "\\begin{align}\n",
        "e_t &= \\mathrm{a}(q, h_t) \\\\\n",
        "\\alpha_t &= \\frac{\\exp(e_t)}{\\sum_k \\exp(e_k)} \\\\\n",
        "c &= \\sum_{t = 1}^T \\alpha_t h_t\n",
        "\\end{align}\n",
        "where $\\mathrm{a}(q, h_t)$ is the attention energy function. Note that c will always be a fixed-length vector (which amounts to a weighted average of the elements of the sequence $h$) regardless of how long the sequence is (i.e. the value of $T$). $\\mathrm{a}(q, h_t)$ can be any function that takes in a single entry of the sequence $h_t$ and outputs an unnormalizes scalar value. One option is to use\n",
        "$$\\mathrm{a}(q, h_t) = q^\\top \\tanh(W_a h_t + b_a)$$\n",
        "where $q \\in \\mathbb{R}^q$, $W_a \\in \\mathbb{R}^{q \\times d}$, and $b_a \\in \\mathbb{R}^q$ are learnable parameters, and $d$ is the dimensionality of $h_t$ (i.e. $h_t \\in \\mathbb{R}^d$).\n",
        "\n",
        "\n",
        "1. Build and train a neural network that uses attention pooling to solve the addition problem. The model should output a scalar which corresponds to the target value for the addition problem (i.e. the sum of the sequence entries that are marked with a \"1\"). Here, \"solved\" means that the squared error of the model's predicitons is always below $0.05$. Use a sequence length of $50$ (which is the default for the `addition_problem` function defined above). *Hints*:\n",
        "  1. This is a regression problem. Your model should predict a continuous scalar value and you can use a squared-error loss.\n",
        "  1. The point of the attention pooling layer is to allow you to put it in an otherwise feed-forward network. So, consider just using simple dense feed-forward layers before and/or after the attention pooling layer. To start, you can try the architecture: feed-forward, attention pooling, feed-forward, output layer.\n",
        "  1. If you are finding that the model is getting stuck at a non-zero squared error, it could be that it's just outputting the mean value and having trouble learning a good solution. Try different initialization, nonlinearities, architecture, learning rate, etc.\n",
        "1. Once you have trained a model that gets solid performance at sequence length $50$, plot the model's average squared error for sequence lengths $50, 55, 65, 80, 100, 125, 150$. You should generate this plot by averaging the squared error over at least $100$ sequences of a given length. Does the model's error get worse (go up) for longer sequences, or does it generalize to longer sequence lengths?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1. Attention Pooling to Solve the Addition Problem \n",
        "\n",
        "Model specs:\n",
        "* Outputs a scalar that corresponds to target value\n",
        "* Squared Error is below 0.05\n",
        "* Use a sequence of length 50"
      ],
      "metadata": {
        "id": "BBlJX1rl7zh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Class Architectures"
      ],
      "metadata": {
        "id": "T67dIFGHEiyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionPooling(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim=2, query_dim=5):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.query_dim = query_dim\n",
        "\n",
        "        self.query_vec = nn.Parameter(torch.rand(query_dim))\n",
        "        self.linear = nn.Linear(hidden_dim, query_dim, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, H):\n",
        "        \"\"\"\n",
        "        The vectorized attention energy function \n",
        "        Currently implemented for bsz of 1.\n",
        "        ========================================\n",
        "\n",
        "        Inputs:\n",
        "        _______\n",
        "\n",
        "        H: The matrix representation of the sequence (seq_len=T, embedding_dim=d)\n",
        "\n",
        "        Outputs:\n",
        "        ________\n",
        "\n",
        "        c: A fixed length vector that has \n",
        "        \"\"\"\n",
        "        # The vector e is of dim (seq_len)\n",
        "        e = self.query_vec @ torch.transpose(torch.tanh(self.linear(H)), -2, -1)\n",
        "        # The vector alpha is of dim (seq_len)\n",
        "        alpha = F.softmax(e, dim=0).unsqueeze(2).expand(-1, -1, self.hidden_dim)\n",
        "        # C returns the weighted averages * values along the seq dimension axis \n",
        "        # (Dim=)\n",
        "        c = (alpha * H).sum(dim=1)\n",
        "        return c\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    # Implements Fully Connected Multilayer Perceptron With a One-Dimensional Scalar Output\n",
        "    def __init__(self, input_dim, m_layers):\n",
        "        super().__init__()\n",
        "        # Get all the power of twos <= input_dim and sample equidistantly from them\n",
        "        self.layer_dims = [input_dim] + list(\n",
        "            reversed(self._sample_list(self._powers_of_two(input_dim - 1), m_layers))\n",
        "        )\n",
        "        self.layers = list(self._get_hidden_layers()) + [\n",
        "            nn.Linear(self.layer_dims[-2], 1)\n",
        "        ]\n",
        "        self.fc = nn.Sequential(*self.layers)\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement forward pass through the neural network\n",
        "        output = self.fc(x)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample_list(list_, n):\n",
        "        \"\"\"\n",
        "        Sample 'n' items from 'list_' in equal distance.\n",
        "        \"\"\"\n",
        "        if n <= 0:\n",
        "            return []\n",
        "        if n >= len(list_):\n",
        "            return list_\n",
        "\n",
        "        step = (len(list_) - 1) / float(n - 1)\n",
        "        indices = [int(round(i * step)) for i in range(n)]\n",
        "        return [list_[i] for i in indices]\n",
        "    \n",
        "    @staticmethod\n",
        "    def _powers_of_two(n):\n",
        "            highest_power = math.floor(math.log2(n))\n",
        "            return [2**i for i in range(highest_power + 1)]\n",
        "        \n",
        "    def _get_hidden_layers(self):\n",
        "        for i in range(len(self.layer_dims) - 2):\n",
        "            yield nn.Linear(self.layer_dims[i], self.layer_dims[i + 1])\n",
        "            yield nn.ReLU()"
      ],
      "metadata": {
        "id": "eL7bFb0V7zRG"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionPoolingNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, stem_layers, head_layers):\n",
        "        super().__init__()\n",
        "        self.stem = [nn.Linear(input_dim, hidden_dim)] + [nn.Linear(hidden_dim, hidden_dim), nn.ReLU()]*(stem_layers-1)\n",
        "        self.ap = [AttentionPooling(hidden_dim)]\n",
        "        self.head = [MLP(hidden_dim, head_layers)]\n",
        "        layers = self.stem + self.ap + self.head\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, H):\n",
        "        \"\"\"\n",
        "        Forward through the network\n",
        "        ========================================\n",
        "\n",
        "        Inputs:\n",
        "        _______\n",
        "\n",
        "        H: The matrix representation of the sequence (seq_len=T, embedding_dim=d)\n",
        "\n",
        "        Outputs:\n",
        "        ________\n",
        "\n",
        "        outs: A scalar that will converge to the sum of the addition problem hopefully \n",
        "        \"\"\"\n",
        "\n",
        "        outs = self.net(H)\n",
        "        return outs\n"
      ],
      "metadata": {
        "id": "HbysSh8VmpYt"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdditionProblemDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequence_length, epoch_size):\n",
        "        self.seq_len = sequence_length\n",
        "        self.data = [addition_problem(self.seq_len) for i in range(epoch_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # Batch is a list of tuples\n",
        "        inputs, labels = zip(*batch)\n",
        "        x = torch.tensor(np.stack(inputs, axis=0), dtype=torch.float32)\n",
        "        y = torch.tensor(np.stack(labels, axis=0), dtype=torch.float32)\n",
        "        return x, y\n",
        "        \n",
        "    def create_dataloader(self, batch_size, shuffle):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset=self,\n",
        "            batch_size=batch_size,\n",
        "            collate_fn=self.collate_fn,\n",
        "            shuffle=shuffle,\n",
        "            pin_memory=True,\n",
        "            drop_last=False\n",
        "        )\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    f(x): \n",
        "    ----- \n",
        "    \"\"\"\n",
        "    def __init__(self, model, loss_fxn, lr, optimizer, scheduler=None, patience=5, device=None):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fxn = loss_fxn\n",
        "        self.lr = lr\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler(self.optimizer) if scheduler is not None else scheduler\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "        self.history = None\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Trainer Object with Loss F(x):{self.loss_fxn}, Optimizer:{self.optimizer.__class__}, Scheduler:{str(self.scheduler.__clas__)},\"\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"2.Perform One Training Step.\"\"\"\n",
        "        inputs, targets = batch[0].to(self.device), batch[1].to(self.device)           \n",
        "        self.optimizer.zero_grad()                                           # Reset gradients along the graph\n",
        "        outs = self.model(inputs)                                            # Get the outputs\n",
        "        loss = self.loss_fxn(outs, targets)                                  # Compute the loss for the batch\n",
        "        loss.backward()                                                      # Back-Prop to Compute Gradients\n",
        "        self.optimizer.step()                                                # Use gradients to update parameters\n",
        "        return loss.detach().item()                                          # NN returns batch average of the loss\n",
        "\n",
        "    def test_step(self, batch, predict=False):\n",
        "        \"\"\"Perform Model Eval for One Batch\"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs, targets = batch[0].to(self.device), batch[1].to(self.device)  \n",
        "            outs = self.model(inputs)                                     # Get the outputs\n",
        "            loss = self.loss_fxn(outs, targets)                                 # Compute the loss for the batch\n",
        "            if predict:\n",
        "                return outs, targets                                            # Return the preds and targets\n",
        "            else:\n",
        "                return loss.detach().item()                                     # NN returns batch average of the loss\n",
        "\n",
        "    def loss_snapshot(self, dataloader):\n",
        "        \"\"\"\n",
        "        Snapshot model loss based on given (criterion)\n",
        "        across the entire dataset during training.\n",
        "        Freeze the model weights to not train during calc of loss.\n",
        "\n",
        "        Inputs:\n",
        "        data_loader : Data Loader object by pytorch. Iterator of tuples\n",
        "                    containing zipped inputs and labels.\n",
        "        \"\"\"\n",
        "        running_loss = 0\n",
        "        dataset_size = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                bsz = len(batch[-1])\n",
        "                running_loss += self.test_step(batch) * bsz                 # Add the non-averaged batch loss\n",
        "                dataset_size += bsz\n",
        "        return running_loss/dataset_size\n",
        "\n",
        "    def acc_snapshot(self, dataloader):\n",
        "        \"\"\"\n",
        "        Snapshot model accuracy [either binarized or continuous score for the purposes\n",
        "        of exploring binary vs continuous trained output] across the entire dataset\n",
        "        during training. Freeze the model weights to not train during calc of loss.\n",
        "\n",
        "        Inputs:\n",
        "        dataloader : Data Loader object by pytorch. Iterator of tuples\n",
        "                    containing dict of inputs and labels.\n",
        "        \"\"\"\n",
        "        num_correct = 0\n",
        "        dataset_size = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                outs, targets = self.test_step(batch, predict=True)\n",
        "                outs = outs.squeeze()\n",
        "                predicted = outs.cpu().numpy()\n",
        "                # Total the number of examples\n",
        "                dataset_size += len(targets)\n",
        "                # Total correct predictions\n",
        "                matches = np.isclose(predicted, targets.cpu().numpy(), atol=5e-2)       \n",
        "                num_correct += np.count_nonzero(matches)\n",
        "        return num_correct/dataset_size\n",
        "\n",
        "    def train_epochs(self, num_epochs, train_loader, val_loader, log_every=10):\n",
        "        setattr(self, 'history', {})\n",
        "        # Instantiate early stopping criteria\n",
        "        best_model = self.model\n",
        "        best_val_loss = np.inf\n",
        "        actual_patience = self.patience\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "\n",
        "        for ep in range(num_epochs):\n",
        "            for i, batch in enumerate(train_loader):                    # Loop through the batches of the dataloader\n",
        "                train_loss = self.train_step(batch)                     # Don't need to save per-step loss\n",
        "\n",
        "            if ep % log_every == 0:                                     # If reached the logging frequency\n",
        "                train_losses += [self.loss_snapshot(train_loader)]           # Snapshot Model Acc and Los\n",
        "                val_losses += [self.loss_snapshot(val_loader)]\n",
        "                train_accs += [self.acc_snapshot(train_loader)]\n",
        "                val_accs += [self.acc_snapshot(val_loader)]\n",
        "                print(\n",
        "                    f\"Epoch: {ep + 1} | \"\n",
        "                    f\"train_loss: {train_losses[-1]:.5f}, \"\n",
        "                    f\"val_loss: {val_losses[-1]:.5f}, \"\n",
        "                    f\"train_acc: {train_accs[-1]:.3f}, \"\n",
        "                    f\"val_acc: {val_accs[-1]:.3f}, \"\n",
        "                    f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}\"\n",
        "                )\n",
        "\n",
        "            # On-epoch End\n",
        "\n",
        "            epoch_val_loss = self.loss_snapshot(val_loader)             # Grab the last val_loss @epoch end\n",
        "            if epoch_val_loss < best_val_loss:                          # Check if current val loss < prev best\n",
        "                best_val_loss = epoch_val_loss                          # Update best_val_loss\n",
        "                best_model = self.model                                 # Cache the best model\n",
        "                actual_patience = self.patience\n",
        "            else:\n",
        "                actual_patience -= 1\n",
        "\n",
        "            if actual_patience == 0:\n",
        "                print(\n",
        "                    f'Stopping after validation loss increased for {self.patience} epochs'\n",
        "                )\n",
        "                break\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()                                   # Step through the LR schedule\n",
        "\n",
        "        self.history[\"train_loss\"] = train_losses\n",
        "        self.history[\"val_loss\"] = val_losses\n",
        "        self.history[\"train_acc\"] = train_accs\n",
        "        self.history[\"val_acc\"] = val_accs\n",
        "\n",
        "        return best_model"
      ],
      "metadata": {
        "id": "_EKKQ2C9lCWe"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Constants\n",
        "\n",
        "SEQ_LEN = 50\n",
        "loss_fxn = nn.MSELoss()\n",
        "lr = .01\n",
        "\n",
        "\n",
        "# Create the Datasets and Data Loaders\n",
        "train_dset = AdditionProblemDataset(sequence_length=SEQ_LEN, epoch_size=10000)\n",
        "val_dset = AdditionProblemDataset(sequence_length=SEQ_LEN, epoch_size=1000)\n",
        "train_loader = train_dset.create_dataloader(64, shuffle=True)\n",
        "val_loader = val_dset.create_dataloader(64, shuffle=False)\n",
        "\n",
        "# Set up the Model\n",
        "\n",
        "model = AttentionPoolingNet(input_dim=2, hidden_dim=16, stem_layers=4, head_layers=4)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "BejSPHTklC6E"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, loss_fxn, lr, optimizer, scheduler=None, patience=100, device=None)\n",
        "opt_model = trainer.train_epochs(1000, train_loader, val_loader, log_every=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfhSCX_h2UW1",
        "outputId": "198a2037-4ac1-4e45-88b3-9eae70af7b33"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.87468, val_loss: 0.82951, train_acc: 2.426, val_acc: 2.856, lr: 1.00E-03\n",
            "Epoch: 11 | train_loss: 0.12531, val_loss: 0.12340, train_acc: 2.447, val_acc: 2.535, lr: 1.00E-03\n",
            "Epoch: 21 | train_loss: 0.09769, val_loss: 0.09642, train_acc: 2.360, val_acc: 2.394, lr: 1.00E-03\n",
            "Epoch: 31 | train_loss: 0.07919, val_loss: 0.08044, train_acc: 2.338, val_acc: 2.411, lr: 1.00E-03\n",
            "Epoch: 41 | train_loss: 0.07073, val_loss: 0.06919, train_acc: 2.377, val_acc: 2.391, lr: 1.00E-03\n",
            "Epoch: 51 | train_loss: 0.05861, val_loss: 0.05469, train_acc: 2.353, val_acc: 2.362, lr: 1.00E-03\n",
            "Epoch: 61 | train_loss: 0.03794, val_loss: 0.03894, train_acc: 2.399, val_acc: 2.390, lr: 1.00E-03\n",
            "Epoch: 71 | train_loss: 0.01783, val_loss: 0.02002, train_acc: 2.512, val_acc: 2.496, lr: 1.00E-03\n",
            "Epoch: 81 | train_loss: 0.00688, val_loss: 0.00882, train_acc: 2.655, val_acc: 2.691, lr: 1.00E-03\n",
            "Epoch: 91 | train_loss: 0.00934, val_loss: 0.01068, train_acc: 2.302, val_acc: 2.354, lr: 1.00E-03\n",
            "Epoch: 101 | train_loss: 0.00108, val_loss: 0.00368, train_acc: 3.047, val_acc: 3.053, lr: 1.00E-03\n",
            "Epoch: 111 | train_loss: 0.00234, val_loss: 0.00402, train_acc: 2.698, val_acc: 2.820, lr: 1.00E-03\n",
            "Epoch: 121 | train_loss: 0.00073, val_loss: 0.00298, train_acc: 3.064, val_acc: 3.099, lr: 1.00E-03\n",
            "Epoch: 131 | train_loss: 0.00358, val_loss: 0.00497, train_acc: 2.443, val_acc: 2.488, lr: 1.00E-03\n",
            "Epoch: 141 | train_loss: 0.00021, val_loss: 0.00222, train_acc: 3.062, val_acc: 3.116, lr: 1.00E-03\n",
            "Epoch: 151 | train_loss: 0.00230, val_loss: 0.00417, train_acc: 2.762, val_acc: 2.802, lr: 1.00E-03\n",
            "Epoch: 161 | train_loss: 0.00882, val_loss: 0.00996, train_acc: 2.110, val_acc: 2.123, lr: 1.00E-03\n",
            "Epoch: 171 | train_loss: 0.00051, val_loss: 0.00288, train_acc: 3.072, val_acc: 3.117, lr: 1.00E-03\n",
            "Epoch: 181 | train_loss: 0.00027, val_loss: 0.00218, train_acc: 3.069, val_acc: 3.126, lr: 1.00E-03\n",
            "Epoch: 191 | train_loss: 0.00761, val_loss: 0.00872, train_acc: 2.163, val_acc: 2.157, lr: 1.00E-03\n",
            "Epoch: 201 | train_loss: 0.00711, val_loss: 0.00838, train_acc: 2.120, val_acc: 2.132, lr: 1.00E-03\n",
            "Epoch: 211 | train_loss: 0.00092, val_loss: 0.00284, train_acc: 3.060, val_acc: 3.128, lr: 1.00E-03\n",
            "Epoch: 221 | train_loss: 0.00501, val_loss: 0.00630, train_acc: 2.233, val_acc: 2.208, lr: 1.00E-03\n",
            "Epoch: 231 | train_loss: 0.00020, val_loss: 0.00228, train_acc: 3.057, val_acc: 3.123, lr: 1.00E-03\n",
            "Epoch: 241 | train_loss: 0.00017, val_loss: 0.00249, train_acc: 3.086, val_acc: 3.125, lr: 1.00E-03\n",
            "Stopping after validation loss increased for 100 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(trainer.history['train_loss'])\n",
        "plt.plot(trainer.history['val_loss'])\n",
        "plt.title('Trainer Loss Curves')\n",
        "plt.xlabel('Epochs [10x]')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "26UxqbsmnMM4",
        "outputId": "c3da5401-3e10-44d2-9aa3-aa3e5e5ceb6e"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAv1klEQVR4nO3deZhcBZn2/+9TVd3V+56NLCQZdhKyAgIqiSuLElEQIrwScVSYAUacUdTfKIzCJTo4Kg46gguKDBH1hRclwACCMKKShD1ANCTBJIQknaT3ruququf3xzndVJruTmepru6u+3NdffWpU6fOeU5VUnef7Tnm7oiIiABE8l2AiIiMHAoFERHppVAQEZFeCgUREemlUBARkV4KBRER6aVQkFHFzO4zs4vyXYfIWKVQkJwzs7asn4yZdWY9vmBf5uXup7v7T3NVazYzu8bMfj4cy+pn2VVm9m0z+1v4Pr0SPm7IRz1SOBQKknPuXtHzA/wNeH/WuNt7pjOzWL5qzOey+zKzYuBh4FjgNKAKOAnYCZywH/MbMesmI59CQfLGzBaZ2WYzu8rMXgd+Yma1ZvZbM9thZrvD4SlZr3nUzP4+HF5mZv9rZjeE024ws9Ozpq02sx+Z2VYz22Jm15pZNOu1fzCzb5nZTuCafaz9LDNbY2ZNYU1HZz13Vbi8VjNba2bvDMefYGarzKzFzLaZ2X8MMPuPAtOAs939RXfPuPt2d/+qu68I5+VmdljWMm81s2sHeV9fMrP3ZU0fC9/j+eHjt5jZE+H6PGtmi7KmXWZm68P12bCvW3cyuigUJN8mAnXAocAnCf5N/iR8PA3oBP5zkNefCKwFGoBvAD8yMwufuxVIAYcB84D3AH/f57XrgQnAdUMt2MyOAO4APg2MA1YAvzGzYjM7ErgMON7dK4H3AhvDl34H+I67VwF/B9w5wCLeBdzv7m1Drakffd/XO4ClWc+/F2h096fMbDJwL3Bt+Jp/AX5tZuPMrBy4ETg9XJ+TgWcOoC4Z4RQKkm8Z4Gp3T7p7p7vvdPdfu3uHu7cSfFmfOsjrX3X3W9w9DfwUmARMMLMJwBnAp9293d23A98Czs967Wvu/l13T7l75z7UfB5wr7s/6O7dwA1AKcEXZhqIA8eYWZG7b3T3V8LXdQOHmVmDu7e5+58GmH89sHUf6unPHu8r8N/AWWZWFj7/EYKgALgQWOHuK8KtkgeBVQTvX8+8ZplZqbtvdfc1B1ibjGAKBcm3He6e6HlgZmVm9gMze9XMWoDHgJqe3T79eL1nwN07wsEKgr+Qi4Ct4S6RJuAHwPis127az5oPAV7NWm4mnNdkd19HsAVxDbDdzJab2SHhpB8HjgBeNrOV2btz+thJEG4HYo/3NazrJeD9YTCcRRAUELxX5/a8T+F79VZgkru3E4TgJQTv5b1mdtQB1iYjmEJB8q1vm95/Bo4ETgx3s7w9HG/sm01AEmhw95rwp8rdjx1k2UP1GsEXaVBYsLtqKrAFwN3/293fGk7jwNfD8X9196UEwfR14Ffh7pm+HgLeO8BzPTqAsqzHE/s839+69exCWgK8GAYFBO/VbVnvU427l7v79WHdD7j7uwmC6mXglkHqklFOoSAjTSXBcYQmM6sDrt6fmbj7VuB/gG+Gp3dGzOzvzGywXVH9iZhZSdZPnOBYwJlm9k4zKyIIsiTwhJkdaWbvCKdLhOuSATCzC81sXLhl0RTOP9PPMm8j+KL+tZkdFdZeb2ZfNLOeXTrPAB8xs6iZncbgu9h6LCc4rnIpb2wlAPycYAviveH8SsKD1VPMbIKZLQkDKgm0DVCzjBEKBRlpvk2wf74R+BNw/wHM66NAMfAisBv4Ffu+W2YpwRd7z88r7r6WYD/8d8M6309wmm0XwfGE68PxrxNsFXwhnNdpwBozayM46Hx+f8cy3D1JcLD5ZeBBoAV4kuBg+p/Dyf4pXG4TcAFw995WJAzKPxIc+/hF1vhNBFsPXwR2EATSZwm+HyLAZwi2jnYRhM+le1uWjF6mm+yIiEgPbSmIiEgvhYKIiPRSKIiISC+FgoiI9Bp1jbIaGhp8+vTp+S5DRGRUWb16daO7j9vbdKMuFKZPn86qVavyXYaIyKhiZq/ufSrtPhIRkSwKBRER6aVQEBGRXqPumIKIDI/u7m42b95MIpHY+8QyYpSUlDBlyhSKior26/UKBRHp1+bNm6msrGT69Om8cd8iGcncnZ07d7J582ZmzJixX/PQ7iMR6VcikaC+vl6BMIqYGfX19Qe0dadQEJEBKRBGnwP9zAomFFZu3MU37n+ZTEZdYUVEBlIwofDspia+9+grtCZT+S5FRIZg586dzJ07l7lz5zJx4kQmT57c+7irq2vQ165atYorrrhir8s4+eSTD0qtjz76KO9730B3Vx1dCuZAc01ZMQBNHV1Ul+7fUXkRGT719fU888wzAFxzzTVUVFTwL//yL73Pp1IpYrH+v8IWLlzIwoUL97qMJ5544qDUOpYUzJZCXXkQBLvaB/8LQ0RGrmXLlnHJJZdw4okn8rnPfY4nn3ySk046iXnz5nHyySezdu1aYM+/3K+55houvvhiFi1axMyZM7nxxht751dRUdE7/aJFizjnnHM46qijuOCCC+i5AdmKFSs46qijWLBgAVdcccU+bRHccccdzJ49m1mzZnHVVVcBkE6nWbZsGbNmzWL27Nl861vfAuDGG2/kmGOO4bjjjuP8888/8DdrPxXglkJ3nisRGX3+7TdrePG1loM6z2MOqeLq9x+7z6/bvHkzTzzxBNFolJaWFh5//HFisRgPPfQQX/ziF/n1r3/9pte8/PLLPPLII7S2tnLkkUdy6aWXvuk8/qeffpo1a9ZwyCGHcMopp/CHP/yBhQsX8qlPfYrHHnuMGTNmsHTp0iHX+dprr3HVVVexevVqamtrec973sPdd9/N1KlT2bJlCy+88AIATU1NAFx//fVs2LCBeDzeOy4fCmZLoTYMhd0d2lIQGc3OPfdcotEoAM3NzZx77rnMmjWLK6+8kjVr1vT7mjPPPJN4PE5DQwPjx49n27Ztb5rmhBNOYMqUKUQiEebOncvGjRt5+eWXmTlzZu85//sSCitXrmTRokWMGzeOWCzGBRdcwGOPPcbMmTNZv349l19+Offffz9VVVUAHHfccVxwwQX8/Oc/H3C32HAomC2F2rLgr4Ld2lIQ2Wf78xd9rpSXl/cOf+lLX2Lx4sXcddddbNy4kUWLFvX7mng83jscjUZJpd58wslQpjkYamtrefbZZ3nggQf4r//6L+68805+/OMfc++99/LYY4/xm9/8huuuu47nn38+L+FQMFsKVSVFRCw40CwiY0NzczOTJ08G4NZbbz3o8z/yyCNZv349GzduBOAXv/jFkF97wgkn8Pvf/57GxkbS6TR33HEHp556Ko2NjWQyGT70oQ9x7bXX8tRTT5HJZNi0aROLFy/m61//Os3NzbS1tR309RmKgtlSiESM6tIi7T4SGUM+97nPcdFFF3Httddy5plnHvT5l5aW8r3vfY/TTjuN8vJyjj/++AGnffjhh5kyZUrv41/+8pdcf/31LF68GHfnzDPPZMmSJTz77LN87GMfI5PJAPC1r32NdDrNhRdeSHNzM+7OFVdcQU1NzUFfn6GwniPso8XChQt9f2+y845vPsrRE6u46YL5B7kqkbHnpZde4uijj853GXnX1tZGRUUF7s4//uM/cvjhh3PllVfmu6xB9ffZmdlqd9/reboFs/uIXRt4b2Qlu9uT+a5EREaRW265hblz53LsscfS3NzMpz71qXyXlFMFs/uIF/8fVzVfy9n2y3xXIiKjyJVXXjnitwwOpsLZUihvCH537MxvHSIiI1jhhEJZPQDRToWCiMhACi4UKtLNdHal81yMiMjIlNNQMLPTzGytma0zs8/38/w0M3vEzJ42s+fM7IycFROGQi2tOi1VRGQAOQsFM4sCNwGnA8cAS83smD6T/Stwp7vPA84HvperenpCoc5aFAoio8DixYt54IEH9hj37W9/m0svvXTA1yxatIieU9bPOOOMfnsIXXPNNdxwww2DLvvuu+/mxRdf7H385S9/mYceemgfqu/faGixncsthROAde6+3t27gOXAkj7TOFAVDlcDr+WsmpJqMhajzlrVFE9kFFi6dCnLly/fY9zy5cuH3H9oxYoV+30BWN9Q+MpXvsK73vWu/ZrXaJPLUJgMbMp6vDkcl+0a4EIz2wysAC7vb0Zm9kkzW2Vmq3bs2LF/1ZiRKa3T7iORUeKcc87h3nvv7b2hzsaNG3nttdd429vexqWXXsrChQs59thjufrqq/t9/fTp02lsbATguuuu44gjjuCtb31rb3ttCK5BOP7445kzZw4f+tCH6Ojo4IknnuCee+7hs5/9LHPnzuWVV15h2bJl/OpXvwKCK5fnzZvH7Nmzufjii0kmk73Lu/rqq5k/fz6zZ8/m5ZdfHvK6jqQW2/m+TmEpcKu7f9PMTgJuM7NZ7p7JnsjdbwZuhuCK5v1eWmkd9a2tbNeWgsi+ue/z8PrzB3eeE2fD6dcP+HRdXR0nnHAC9913H0uWLGH58uV8+MMfxsy47rrrqKurI51O8853vpPnnnuO4447rt/5rF69muXLl/PMM8+QSqWYP38+CxYsAOCDH/wgn/jEJwD413/9V370ox9x+eWXc9ZZZ/G+972Pc845Z495JRIJli1bxsMPP8wRRxzBRz/6Ub7//e/z6U9/GoCGhgaeeuopvve973HDDTfwwx/+cK9vw0hrsZ3LLYUtwNSsx1PCcdk+DtwJ4O5/BEqAhlwVFKlooNZaadKNdkRGhexdSNm7ju68807mz5/PvHnzWLNmzR67evp6/PHHOfvssykrK6Oqqoqzzjqr97kXXniBt73tbcyePZvbb799wNbbPdauXcuMGTM44ogjALjooot47LHHep//4Ac/CMCCBQt6m+jtzUhrsZ3LLYWVwOFmNoMgDM4HPtJnmr8B7wRuNbOjCUJhP/cP7V2kvIEG28Au7T4S2TeD/EWfS0uWLOHKK6/kqaeeoqOjgwULFrBhwwZuuOEGVq5cSW1tLcuWLSORSOzX/JctW8bdd9/NnDlzuPXWW3n00UcPqN6e9tsHo/V2vlps52xLwd1TwGXAA8BLBGcZrTGzr5hZT1T/M/AJM3sWuANY5rns0FdWT5216ECzyChRUVHB4sWLufjii3u3ElpaWigvL6e6uppt27Zx3333DTqPt7/97dx99910dnbS2trKb37zm97nWltbmTRpEt3d3dx+++294ysrK2ltbX3TvI488kg2btzIunXrALjttts49dRTD2gdR1qL7ZweU3D3FQQHkLPHfTlr+EXglFzWsIeyeqpoo6m9c9gWKSIHZunSpZx99tm9u5HmzJnDvHnzOOqoo5g6dSqnnDL4V8j8+fM577zzmDNnDuPHj9+j/fVXv/pVTjzxRMaNG8eJJ57YGwTnn38+n/jEJ7jxxht7DzADlJSU8JOf/IRzzz2XVCrF8ccfzyWXXLJP6zPSW2wXVOts/vwDuO9zXFh/Bz+/PHfXyYmMBWqdPXqpdfZQhRewRToa81yIiMjIVGChUAdApHNXngsRERmZCiwUgrNd411NpNKZvUwsIqNt97Ic+GdWYKHwRv+jpk6dgSQymJKSEnbu3KlgGEXcnZ07d1JSUrLf88j3Fc3DK6tTalNHFw0V8TwXJDJyTZkyhc2bN7PfrWUkL0pKSvY4u2lfFVYoFJWQipVTl2plt65VEBlUUVERM2bMyHcZMswKa/cRkCmpo85a2a1WFyIib1JwoUBZHXWofbaISH8KLhQiFeOotVb1PxIR6UfBhUK0op563X1NRKRfBRcKVtZAnbXR1K7dRyIifRVcKFBeTxkJ2tre3AFRRKTQFV4ohNcqZNp35rkQEZGRpwBDIWh14WqKJyLyJgUYCsGWQrRTWwoiIn0VbCgUdzWpp4uISB+FFwrlwe6jam+hNXlg91AVERlrCi8USqpxIkGnVJ2WKiKyh8ILhUiU7ng1dbTqAjYRkT4KLxSAdEk9taZQEBHpqyBDgbJ66hUKIiJvUpChEK1ooJZWduuYgojIHgoyFGKV46iz4O5rIiLyhoIMhUh5cEyhqT2Z71JEREaUggwFyuqJkaGzbXe+KxERGVEKNBSCC9gybWp1ISKSrUBDIWh1gZriiYjsoUBDoQ5QUzwRkb4KMxTC/kdFXU35rUNEZIQpzFAIdx9VpptIdKfzXIyIyMhRmKFQVEYqEg9OS+3QBWwiIj0KMxTM6I7XqSmeiEgfhRkKQLq0jjprZXe7QkFEpEfBhoKVhaGg3UciIr0KNhQiFeOCpnjafSQi0iunoWBmp5nZWjNbZ2afH2CaD5vZi2a2xsz+O5f1ZCuqbAjuvqZQEBHpFcvVjM0sCtwEvBvYDKw0s3vc/cWsaQ4HvgCc4u67zWx8rurpK1YxjirrpLmtY7gWKSIy4uVyS+EEYJ27r3f3LmA5sKTPNJ8AbnL33QDuvj2H9ewpvFahq0WtLkREeuQyFCYDm7Iebw7HZTsCOMLM/mBmfzKz0/qbkZl90sxWmdmqHTt2HJzqwlDItB+k+YmIjAH5PtAcAw4HFgFLgVvMrKbvRO5+s7svdPeF48aNOzhL7m2Kp/5HIiI9chkKW4CpWY+nhOOybQbucfdud98A/IUgJHIv7H8UTewalsWJiIwGuQyFlcDhZjbDzIqB84F7+kxzN8FWAmbWQLA7aX0Oa3pDuKVQnNSNdkREeuQsFNw9BVwGPAC8BNzp7mvM7CtmdlY42QPATjN7EXgE+Ky7D8/+nNLa4FeqiVQ6MyyLFBEZ6XJ2SiqAu68AVvQZ9+WsYQc+E/4Mr2gRyVgVtalWmju7qa+ID3sJIiIjTb4PNOdVd7yWemtRqwsRkVBBh0K6tI5aWnVVs4hIqKBDgbIG6qxNWwoiIqGCDoVoRdD/SE3xREQCBR0KRZUNwe6j9mS+SxERGRFyevbRSFdcNR6zFG2tzfkuRURkRCjoLQULL2BLtar/kYgIFHgo9LS6SLepU6qICBR6KKgpnojIHgo8FOoAiCUUCiIiMIRQMLNTzKw8HL7QzP7DzA7NfWnDoCzYfVSUbMpvHSIiI8RQthS+D3SY2Rzgn4FXgJ/ltKrhEq8kbTFKuncTtGESESlsQwmFVNi4bgnwn+5+E1CZ27KGiRmJohpqvIW2ZCrf1YiI5N1QQqHVzL4AXAjca2YRoCi3ZQ2f7ngdddZKk1pdiIgMKRTOA5LAx939dYI7qP17TqsaRunSIBTU6kJEZGhXNLcC33H3tJkdARwF3JHbsoaPldVTy6ts1paCiMiQthQeA+JmNhn4H+D/ALfmsqjhFKkYF+4+0paCiMhQQsHcvQP4IPA9dz8XmJXbsoZPvGoctdbG7taOfJciIpJ3QwoFMzsJuAC4dx9eNyrEq8YB0NmiC9hERIby5f5p4AvAXe6+xsxmAo/ktKphFKkILmBTUzwRkSEcaHb33wO/N7MKM6tw9/XAFbkvbZiE/Y8y7WqKJyIylDYXs83saWAN8KKZrTazY3Nf2jBRUzwRkV5D2X30A+Az7n6ou08jaHVxS27LGkZh/6NoYleeCxERyb+hhEK5u/ceQ3D3R4HynFU03MJOqcXJ3XkuREQk/4Zy8dp6M/sScFv4+EJgfe5KGmaxOMlIGSXdCgURkaFsKVwMjAP+L/BroAH4WC6LGm6J4loqMy0kU+l8lyIikldDOftoN33ONjKzXxD0RBoTuuO11Le30NTRzYSqaL7LERHJm/29CO2kg1pFnmVK6qlVUzwRkbFzZfIBKa8PWl20qymeiBS2AXcfmdn8gZ5iDN1PASBa0UA9LTynLQURKXCDHVP45iDPvXywC8mn4spxlFoXLa3NwKR8lyMikjcDhoK7Lx7OQvKppGY8AMnmHQS3ixARKUw6pgAUVQadUrtb1BRPRAqbQgGymuKp/5GIFDaFAvT2P1JTPBEpdAOGgpldmDV8Sp/nLstlUcMu7H8UTSgURKSwDbal8Jms4e/2ee7ioczczE4zs7Vmts7MPj/IdB8yMzezhUOZ70FXUkOaiJriiUjBGywUbIDh/h6/+cVmUeAm4HTgGGCpmR3Tz3SVwD8Bf95rtbkSidAZq6akuylvJYiIjASDhYIPMNzf4/6cAKxz9/Xu3gUsB5b0M91Xga8DiSHMM2cSRTVUpJtIZ4ayaiIiY9NgoXCUmT1nZs9nDfc8PnII854MbMp6vDkc1yu8anqqu9+7r4UfbN3xOuqslZZOtboQkcI12BXNR+dywWYWAf4DWDaEaT8JfBJg2rRpOaknU1pHLa+zu6OL2vLinCxDRGSkG3BLwd1fzf4B2oD5QEP4eG+2AFOzHk8Jx/WoBGYBj5rZRuAtwD39HWx295vdfaG7Lxw3btwQFr3vrLyeOnVKFZECN9gpqb81s1nh8CTgBYKzjm4zs08PYd4rgcPNbIaZFQPnA/f0POnuze7e4O7T3X068CfgLHdftd9rcwCi5Q3U0srutmQ+Fi8iMiIMdkxhhru/EA5/DHjQ3d8PnMgQTkl19xRwGfAA8BJwp7uvMbOvmNlZB1j3QVdcNZ6oOW3NulZBRArXYMcUso+4vhO4BcDdW80sM5SZu/sKYEWfcV8eYNpFQ5lnrsRrgt1SXc3b81mGiEheDRYKm8zscoKzhuYD9wOYWSlj7H4KAKXVEwDoblNTPBEpXIPtPvo4cCzB2UHnuXtTOP4twE9yW9bws7ApXrpNu49EpHANdj+F7cAl/Yx/BHgkl0XlRRgK1tGY50JERPJnsNtx3jPQcwDuPuIOFh+QMBSiiV15LkREJH8GO6ZwEsEVyXcQ9CXaa7+jUa24jKSVqCmeiBS0wUJhIvBuYCnwEeBe4A53XzMcheVDR6yaUjXFE5ECNtgVzWl3v9/dLyI4uLyO4OrjsXUvhSyJ4lrK0824qymeiBSmwbYUMLM4cCbB1sJ04EbgrtyXlR/d8VpqWrfT0ZWmPD7oWyMiMiYNdqD5ZwS9iVYA/5Z1dfOY5SV11LGOXe1dCgURKUiDXadwIXA4wQ1wnjCzlvCn1cxahqe8YVbeQK210dSh9tkiUpgGu05hsMAYk6IVDVRaJ02trUB1vssRERl2BffFP5jiqqD/UUeT+h+JSGFSKGQpqR4PQLJF/Y9EpDApFLKU1QSh0N2qLQURKUwKhSyximD3UUZN8USkQCkUspU3BL87FAoiUpgUCtlKashgxNQUT0QKlEIhWzRGR6RCTfFEpGApFPpoj9VSkmrKdxkiInmhUOgjWVRDhUJBRAqUQqGPVEktVd5CVyqT71JERIadQqGPdGk9ddZKU0dXvksRERl2CoU+rKyeWlrZ3a5QEJHCo1DoI1rRQLGlaWnWtQoiUngUCn0UqSmeiBQwhUIfpT1N8ZoVCiJSeBQKfZTXTgCgq1WdUkWk8CgU+uhpn+3tOqYgIoVHodBXWT0ApqZ4IlKAFAp9FVfQTYyomuKJSAFSKPRlRkukRk3xRKQgKRT60RmrprRboSAihUeh0I9kcQ3l6eZ8lyEiMuwUCv3oLqmnKtNCJuP5LkVEZFgpFPqRLqmjzlpoSXTnuxQRkWGlUOiHlddTbR3sbu3IdykiIsNKodCPaEXQ/6hV/Y9EpMDkNBTM7DQzW2tm68zs8/08/xkze9HMnjOzh83s0FzWM1TFlQ0AdOzaludKRESGV85CwcyiwE3A6cAxwFIzO6bPZE8DC939OOBXwDdyVc++KA37HyVb1P9IRApLLrcUTgDWuft6d+8ClgNLsidw90fcvWfH/Z+AKTmsZ8h6muJ1qymeiBSYXIbCZGBT1uPN4biBfBy4r78nzOyTZrbKzFbt2JH7L+qKmp6meI05X5aIyEgyIg40m9mFwELg3/t73t1vdveF7r5w3Lhxua+nPDimQKf6H4lIYYnlcN5bgKlZj6eE4/ZgZu8C/j/gVHdP5rCeoYsW0UY5MTXFE5ECk8sthZXA4WY2w8yKgfOBe7InMLN5wA+As9x9RJ3/2RqtojipUBCRwpKzUHD3FHAZ8ADwEnCnu68xs6+Y2VnhZP8OVAC/NLNnzOyeAWY37DpiNZR0N+W7DBGRYZXL3Ue4+wpgRZ9xX84aflcul38gksW1lLdtzXcZIiLDakQcaB6JUvFaqjPNuKspnogUDoXCANKl9dTSSmdXKt+liIgMG4XCAKysnhLrpqlF91UQkcKhUBhAtCK4VqF15+t5rkREZPgoFAYQrw6uau5Qp1QRKSAKhQGUhqGQUFM8ESkgCoUB9DTFS7dqS0FECodCYQCV9RMByLTtzHMlIiLDR6EwgKKyGlIegQ6FgogUDoXCQMxotio1xRORgqJQGER7rIbK5pe59cFVtCd1EZuIjH0KhUGUzj+fY3mFD//vGdz1tY9y6/1P0JrozndZIiI5Y6Ott8/ChQt91apVw7fAHWvZ+cD11Ky7m5RHuMcW0378ZZz9jlOoLi0avjpERA6Ama1294V7nU6hMES7N7LzgX+n6uVfYJ5mhb2VXfMu4wPvXkxNWfHw1yMisg8UCrnSspXGB79J5Qu3UZRJ8hAnsPW4f+D9p51BXbnCQURGJoVCrrXvpPHhb1H2zI8py7TzmM9l47GXcsYZZ9NQEc93dSIie1AoDJdEM42/u4mS1f9FRbqZJ/1o1k16P4csfB9vmTOLkqJovisUEVEoDLuudnb8/mZiT36f2u5tALzsh/K3+lOomnU6c056D6WlJXkuUkQKlUIhX9zp3vo8m5+8B//rg0xtf54i0rR6Kesqjyd6xHs4/JQllNZPy3elIlJAFAojRKp9N3/98wraXriPabv+wASCK6Q3F88kcehiJh9/FqV/dwpEdXqriOSOQmEESqczPP/0n3j9qd9Qv/Ux5mReotjSJK2E7RVH0tkwm6Kp86k97ERqphwNER2PEJGDQ6EwwqUzztPrNrHuT78lvvkPTEv8haPtVcosCUAHcTYUHUZj5TF0jT+O+NQFTJg5i0MbKnTwWkT2mUJhlEmlM2ze2cr29c/T+bdVFG1/nobmNUzrfoUSugBo8xLW+HQ2FB3O7ro5xGaeytGHzeS4qdVUlWj3k4gMTKEwVqRTdGx9iV1//TNdf1tNSePzNLStpdiDoHgpM5UnMrPYULkAO/RkjpoxhblTazhyQiWxqFpbiUhAoTCWpVOw9VkSf/kdnX95hMrtq4hlkqSJ8HxmBk9kjmW1zSY5+XiOnTaRuVNrmDetlonVOiVWpFApFApJKgmbV+Lrf0/yr49S/PpTRDxFNzGezhzOH9LH8ETmWLonzWPJghmcNecQ6nXVtUhBUSgUsmQb/O1PsOH3ZDY8hm19FsPpsFIeTx3L4z6H7umLWXTiQt5x9HjiMR24FhnrFAryhs7dsPF/Yd3DdK99kKK2zQC8kpnEHyPzSM14B3Pe9j7mzpiImeW5WBHJBYWC9M8dGv9K5q8P0vT8fVS8/meKvYuEF/FcbBaJQxdzxCkfYOLM40ABITJmKBRkaLo76Vz3OJuevIfyTY8yObUJgB3R8TRPWcQhJ51P2eGnQjSW3zpF5IAoFGS/bH11LS8/fhexDb9jfuoZyi1Ja7SG5umnM+Gkj1A08xRdaS0yCikU5IC4O89seJ21j/+a2o338rbMasosSWusjs7DzmTcW87Hpp0MEV0LITIaKBTkoOlOZ3jipb/xyhN3cciW+ziVpym1LtqKGkgdtYSa4z8MU05QQIiMYAoFyYm2ZIqHnnmFLU/exWHbH2JR5Bni1k1bfAKRWR+g7Ngz4ZC5UFKd71JFJItCQXJue0uC+1b/hR2r72ZO86O8PfIscUsB0Fp+KD5pLuXTFxCdPA8mzVFQiOSRQkGG1brtraxY+Rea//oHKne9wFG+nlmRDUyxxt5pWsumkZ44h4oZC4hNnh8ERWnNwDNNd0MqAd2J4HfPj2egajKU1eu02VzIZMDTusfHGDMiQsHMTgO+A0SBH7r79X2ejwM/AxYAO4Hz3H3jYPNUKIx86YyzobGNF7a0sH7jRhKbnqJ85wsckXmF2X2DonQK6Vgplk4SSSeJppPEMkmimS6ipAddTpeV0FY6iXTVVIrqp1M+YSZFdYdCzaFQMw3KGxQafSVboWUrtG4l1fwarTs2kdi1hXTzFqJt2yhJbKOyeycR0rTGGugon4xXTyM+bgZVE2dSVD8jeG+rp+QuNDJpaNsGzZvp2vk3mrdtINH4Kt60iWhiNw5gkfCzjeAWwczw8DFm4U8k+CmuJFY9iZK6Q6hsmEqsehJUToSKCRCv2L8a3aGrDRLNwU8qGfyBU1oL8eoReXwt76FgZlHgL8C7gc3ASmCpu7+YNc0/AMe5+yVmdj5wtrufN9h8FQqjUybjbNrdwQtbWnjl1VdJvLqa0p3PMyO1nmJSJCgm6UUkrZhMJE46GicTjeOxUojFIVaCFZVgRWXBd0HzForbNlOf2sYU28EU20GNte+xzFSkhGTFZCK1hxKrqMfdyWQyZDIZ3MPfGSfj6eB3Jo27457BMxmwCBYrxiJFWKyISDRGJFpEJFZEJFZMJFpEtKiIaDhskQjp7iTdiQ5SyQ5SXR1kkh1kujvxrk5IJbBUJ5ZKEkkniKYTGNAdLSVdVE6mqBwvrsCKK4iUVBArqSRWVkW8tJKisiqsuCL4EvPMHltP6a4OupMdpJMdZLo6SXd1kunqxFMJ6O6ErnaiHTsoTWwnnul402fT4qVs91q2eS27Y/Uk4uPJRIsp7XiNceng/Z3ETqL2xndFhggdJeNJVU2lqG46peNnECmtAYsGpyxbBCKxcDh7XDQYb1HAybS8TvuOV4Mv/ebNxNu3UJHc/qY/CFq9lC3ewC4qATCHiGUAiJAhgmM4ERzC34YTxamkg/HWRNy637TuiUgZifg4UmXjoXIC8drJlNRMJNWVoKttN90du/HOJjzRTCTZQqyrheJUGyXpNiJk+v+3ToREtJJkcTXpeA2ZklqiZbXEKhuIV9YTr2zAyuogVgJ4EDAD/mbPx5MXQMNhe/vv1q+REAonAde4+3vDx18AcPevZU3zQDjNH80sBrwOjPNBilIojB3uzraWJGl3yoqilMWjFEcj+9RqozXRzcbGDtY3trHl9W20vr6B7p0bibZuoiE7MGgng5HBwq+L4CeT9Zs9nocIToz0Gz+Wpog0MVLEyITjUxTbnl9gnV5MgmI6KSbhxSQJHie8mARFJCimi2LS0RIwoyjdQYknqCBBmSWooJMyS1JOJ+Uk9vgyHkzSY1nLCpYT1BFnh1ez0+pIlI4nUz6RSNUhxOsnUzVuKhMaGjikppRJ1SVvuoFTc2c3Gxvb2bi9iR2vbaBj+wYyuzZS3LaZCZltTLHGMDR2ERlinX11e5Rt1LLFG9jq9bTEJ9JdfghWM5XShmlUTZzBpAkTmFZXRn15cbBV4E7GIeNOOuN4z7A7ngmGex43d3SztamTXY3baWncRFfTVtItW4m1b6M4sYPa9C7GWRMT2M14a+q90VWrl9JKKS1eTgtltHgZrZSTjFbQXVRJOl6Fx6uxkmoisWLSnc3QuYtYoomi7ibK0y1U006NtVFLG9XWRpV17td71OO5uVdz3Ac+s1+vHQmhcA5wmrv/ffj4/wAnuvtlWdO8EE6zOXz8SjhNY595fRL4JMC0adMWvPrqqzmpWcYOd2dXexcbGttZ39hOU0cXRdEIRdEIxdEIRTEjFgkfx4yiaIRYZM/hjDvJVIZkKk1XKkNXKkMy/N2VzpDsTtOVztDVnaYrlSKVSlMSj1NeUkRFPEp5PEZ5PEZFPEZ5cfg7HB+PvRF+Hi6nJdFNS2eK1kQ3LYnwd0c3nR2tJNtbSHa2kOpsBSJE4qVEisqIxUuJFZdSXFJKcXExpUVRSooi4e/gp6w4ysSqEmrKig5abyt3Z0drkg2N7WxobOfV7U0kE21YJoORxjLhD2nMM+AZzHvGZTBPEQHidYdQP2EqU+oqmVZXxqSaEoqG+T4gbckUrzcneL05wWtNHexubqa4uITqilJqy4qpKSvq/V1VUkQkMrT3MJlKs7u9m8a2JDvbu9jZlmRXSzvtLY0kmhvJdCfxcFYe/lHiWLBRQM/usOAPlJ7n3n38LE6ZNXO/1nOooTAqehe4+83AzRBsKeS5HBkFzIz6ijj1FXEWTq/LdzmDMrPeL/DxlfmuZmjMjPFVJYyvKuHEmfXAtHyXtN8q4jEOG1/BYeP38/jCAOKxKBOro/3cx+TIg7qcgy2XkbwFmJr1eEo4rt9pwt1H1QQHnEVEJA9yGQorgcPNbIaZFQPnA/f0meYe4KJw+Bzgd4MdTxARkdzK2e4jd0+Z2WXAAwSnpP7Y3deY2VeAVe5+D/Aj4DYzWwfsIggOERHJk5weU3D3FcCKPuO+nDWcAM7NZQ0iIjJ0I+8KCxERyRuFgoiI9FIoiIhIL4WCiIj0GnVdUs1sB7C/lzQ3AI17nWrsKuT1L+R1h8Jef6174FB3H7e3F4y6UDgQZrZqKJd5j1WFvP6FvO5Q2Ouvdd+3ddfuIxER6aVQEBGRXoUWCjfnu4A8K+T1L+R1h8Jef637PiioYwoiIjK4QttSEBGRQSgURESkV8GEgpmdZmZrzWydmX0+3/UMJzPbaGbPm9kzZjbm72VqZj82s+3hnf16xtWZ2YNm9tfwd20+a8yVAdb9GjPbEn7+z5jZGfmsMVfMbKqZPWJmL5rZGjP7p3B8oXz2A63/Pn3+BXFMwcyiwF+AdwObCe71sNTdX8xrYcPEzDYCC/ve5nSsMrO3A23Az9x9VjjuG8Aud78+/KOg1t2vymeduTDAul8DtLn7DfmsLdfMbBIwyd2fMrNKYDXwAWAZhfHZD7T+H2YfPv9C2VI4AVjn7uvdvQtYDizJc02SI+7+GMH9ObItAX4aDv+U4D/LmDPAuhcEd9/q7k+Fw63AS8BkCuezH2j990mhhMJkYFPW483sx5s1ijnwP2a22sw+me9i8mSCu28Nh18HJuSzmDy4zMyeC3cvjcndJ9nMbDowD/gzBfjZ91l/2IfPv1BCodC91d3nA6cD/xjuYihY4S1fx/5+0zd8H/g7YC6wFfhmXqvJMTOrAH4NfNrdW7KfK4TPvp/136fPv1BCYQswNevxlHBcQXD3LeHv7cBdBLvTCs22cJ9rz77X7XmuZ9i4+zZ3T7t7BriFMfz5m1kRwRfi7e7+f8PRBfPZ97f++/r5F0oorAQON7MZZlZMcC/oe/Jc07Aws/LwoBNmVg68B3hh8FeNSfcAF4XDFwH/L4+1DKueL8TQ2YzRz9/MjOC+7y+5+39kPVUQn/1A67+vn39BnH0EEJ6G9W0gCvzY3a/Lb0XDw8xmEmwdQHBP7v8e6+tuZncAiwjaBm8DrgbuBu4EphG0Xv+wu4+5A7IDrPsigl0HDmwEPpW1j33MMLO3Ao8DzwOZcPQXCfarF8JnP9D6L2UfPv+CCQUREdm7Qtl9JCIiQ6BQEBGRXgoFERHppVAQEZFeCgUREemlUJAxxczSWd0gnzmYHXHNbHp299F9fO0yM9thZj8MH9eHHS3bzOw/+0y7IOxqu87MbgzPPx9s3reb2S4zO2d/ahPJFst3ASIHWae7z813EQP4hbtfFg4ngC8Bs8KfbN8HPkFwfv0K4DTgvoFm6u4XmNmtB71aKUjaUpCCEN5T4hvhX+BPmtlh4fjpZva7sFnYw2Y2LRw/wczuMrNnw5+Tw1lFzeyWsF/9/5hZaTj9FWEf++fMbPne6nH3dnf/X4JwyK5zElDl7n8K+/T8DPiAmcXMbKWZLQqn+5qZjemLECU/FAoy1pT22X10XtZzze4+G/hPgqvbAb4L/NTdjwNuB24Mx98I/N7d5wDzgTXh+MOBm9z9WKAJ+FA4/vPAvHA+lxxA/ZMJuvj22AxMdvcUwX0Bvm9m7yLYevi3A1iOSL+0+0jGmsF2H92R9ftb4fBJwAfD4duAb4TD7wA+CuDuaaA5bDm8wd2fCadZDUwPh58Dbjezuwlaahx07r7GzG4DfgucFN4bROSg0paCFBIfYHhfJLOG07zxh9WZwE0EWxUrzWx//+DaQtDFt0ffjr6zCbZQxu/n/EUGpVCQQnJe1u8/hsNPEHTNBbiAoKEYwMPApRDcztXMqgeaqZlFgKnu/ghwFVANVOxPgWGjshYze0t41tFHCbt6mtkHgTrg7cB3zaxmf5YhMhjtPpKxptTMnsl6fL+795yWWmtmzxH8tb80HHc58BMz+yywA/hYOP6fgJvN7OMEWwSXEtygpD9R4OdhcBhwo7s37a3Q8N7ZVUCxmX0AeE943/B/AG4FSgnOOrrPzBqA64F3uvum8DTW7/BGS2iRg0JdUqUghF/AC929MU/LXxYu/7K9Tbuf878V+K27/yoX85fCod1HIsOjEzi95+K1g8nMbgdOpc/prSL7Q1sKIiLSS1sKIiLSS6EgIiK9FAoiItJLoSAiIr0UCiIi0uv/B+UReVx7M/vjAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_x, sample_y = addition_problem(50)\n",
        "x = torch.tensor(sample_x, dtype=torch.float32).unsqueeze(0)"
      ],
      "metadata": {
        "id": "rOV9C_conNgm"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfdmmv5SnXjJ",
        "outputId": "f274c94b-888a-4ec8-c2de-938b16381e1e"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12.2299]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSXLG5QXoD6h",
        "outputId": "07f91952-283c-41db-b16e-bb1174297320"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.69461102])"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.sum(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRXH7-BQoUpJ",
        "outputId": "b906956e-4c7a-4d47-ff30-be54cbae4b0b"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4327, -0.9616, -0.8102, -0.0793, -0.5470,  0.0204, -0.4467,  0.6626,\n",
              "         -0.7826, -0.7224,  0.3583,  0.5303, -0.6930, -0.5828,  0.5424, -0.4095,\n",
              "          0.2433, -0.9866,  0.0056,  0.9631,  0.4130, -0.4019,  0.3124,  0.0021,\n",
              "         -0.4272, -0.4639,  0.8382, -0.9234,  0.7764,  0.5609,  0.0057,  0.0844,\n",
              "          0.3062,  0.6828, -0.8319,  0.0846, -0.4475,  0.8723, -0.9312,  0.4161,\n",
              "          0.6891,  0.4109, -0.6990,  1.1018, -0.6063,  0.9669,  1.5928,  0.7752,\n",
              "          0.7654, -0.7487]])"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disclaimer:\n",
        "\n",
        "Unfortunately, I was not able to debug the issue with the code due to circumstantial reasons. Therefore submitting as is in hopes that some readily apparent stupid mistake is found during grading. "
      ],
      "metadata": {
        "id": "gJafraKgKdfc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATx1ILNQuxUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}