{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc7730a",
   "metadata": {},
   "source": [
    "# Chapter 3: Linear Neural Networks for Regression\n",
    "\n",
    "### Dhuvi Karthikeyan\n",
    "\n",
    "##### 1/11/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43b952",
   "metadata": {},
   "source": [
    "## 3.1 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b55935",
   "metadata": {},
   "source": [
    "Minimal notes just on the things I had forgotten/wish to review for retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259f96f",
   "metadata": {},
   "source": [
    "Affine transformation that decomposes into a linear transformation by weighted sum fo input features and then translation by the addition of a bias term. \n",
    "\n",
    "Conditional mean: $E[Y|X=x]$ -> MLE Estimates\n",
    "\n",
    "$$ y = \\textbf{w}^Tx + b + \\epsilon ~ N(0, \\sigma^2)$$\n",
    "\n",
    "$$ P(y | x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{(-\\frac{1}{2\\sigma^2}(y - \\textbf{w}^Tx -b)^2)}$$ \n",
    "\n",
    "$$ P(y | X) = \\prod_{i=1}^np(y^{(i)}|x^{(i)}) $$\n",
    "\n",
    "$$ -logP(y|X) = \\sum_{i=1}^n \\frac{1}{2}log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}(y^{(i)} - \\textbf{w}^Tx^{(i)} - b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b621e",
   "metadata": {},
   "source": [
    "## 3.2 Object-Oriented Design for Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89723a21",
   "metadata": {},
   "source": [
    "### 3.2.1 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7ec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function for adding attributes to objects after instantiation\n",
    "\n",
    "def add_to_class(Class):\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d88c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:\n",
    "    def __init__(self):\n",
    "        self.instantiated = True\n",
    "        \n",
    "c = C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e06bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(C)                  # Limits the scope of adding attributes to just the class\n",
    "def add_name(self, name):\n",
    "    self.name = name\n",
    "    \n",
    "def add_directive(obj, directive): # Global scope to all objects [Not best practices]\n",
    "    obj.directive = directive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02ad3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.add_name('ClassC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f1478f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ClassC'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b08d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_directive(c, \"Exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39167296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exist'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.directive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6078e6",
   "metadata": {},
   "source": [
    "## 3.4 Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01ebd9",
   "metadata": {},
   "source": [
    "This is the main problem from HW1 and is pulled from my implementation in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baf11f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoader Class\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Implements a Data Loading Class for passing mini-batches to model after\n",
    "    minor processing to ensure that matrix multiplication works. Assumes shuffle\n",
    "    to be true.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "          self, \n",
    "          inputs,\n",
    "          labels,\n",
    "          batch_size\n",
    "    ):\n",
    "        self.__len__ = len(labels)\n",
    "        self.data = self.gen_tensor(inputs)\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.called_idx = np.array([], dtype=int)\n",
    "        self.prefetch = self.__get_idx__()\n",
    "\n",
    "  \n",
    "    def reshape_input(self, inputs):\n",
    "        '''Reshapes input of dims >= 1 to a matrix of n x n_features'''\n",
    "        n_examples = inputs.shape[0] #Assumes first dim is n_examples\n",
    "        assert n_examples == self.__len__\n",
    "        return inputs.reshape(n_examples, int(inputs.size/n_examples))\n",
    "\n",
    "        def gen_tensor(self, inputs):\n",
    "            '''Checks for iterable inputs and runs reshape above''' \n",
    "            try:\n",
    "                iter(inputs)\n",
    "            except TypeError:\n",
    "                print(\"Inputs is not an iterable.\")\n",
    "            return self.reshape_input(inputs)\n",
    "\n",
    "    def __get_idx__(self):\n",
    "        '''Check that we have enough examples for another batch_size'''\n",
    "        if self.__len__ - len(self.called_idx) < self.batch_size:\n",
    "          # On epoch end reset the called indices\n",
    "          self.called_idx = np.array([], dtype=int)\n",
    "        remaining_idx = np.delete(np.arange(self.__len__), list(self.called_idx))\n",
    "        idx = np.random.choice(remaining_idx, self.batch_size, replace=False)\n",
    "        self.called_idx = np.append(self.called_idx, idx)\n",
    "        return idx\n",
    "\n",
    "    def _get_item_(self):\n",
    "        '''Get a batch of the data when called with batch_size'''\n",
    "        sampled_idx = self.__get_idx__()\n",
    "        return (self.data[sampled_idx, :], self.labels[sampled_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81041d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LinearRegression Model Class\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Implements a Linear Regression Model Class for performing linear regression\n",
    "    without the closed form solution. Randomly initializes weights and biases \n",
    "    before using mini-batch SGD to optimize weights\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self,\n",
    "      input_dim\n",
    "    ):\n",
    "        self.weights = np.random.randn(input_dim)\n",
    "        self.bias = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #Vectorized implementation of Forward Call w// broadcasting for bias\n",
    "        return X @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b084f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gradient Descent Class\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Implements gradient descent algorithm. Works with arbitrary models\n",
    "    and loss functions (whose gradients are manually implemented).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model, \n",
    "        learning_rate,\n",
    "        grad_fxn\n",
    "    ):\n",
    "        self.eta = learning_rate\n",
    "        self.grad = grad_fxn\n",
    "        self.model = model\n",
    "\n",
    "    def step(self, inputs, preds, labels):\n",
    "        grads = self.grad(inputs, preds, labels)\n",
    "        self.update_params(grads)\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        params = self.model.params.keys()\n",
    "        for i,p in enumerate(params):\n",
    "            self.model.params[p] -= self.eta*grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841df9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Train_Iters Function\n",
    "def train_iters(iterations, data_loader, model, lossfn, optimizer):\n",
    "    running_loss = 0\n",
    "    loss = []\n",
    "    for i in tqdm(range(iterations)):\n",
    "        x,y = data_loader._get_item_()\n",
    "        preds = model.forward(x)\n",
    "        running_loss += lossfn(preds, y)\n",
    "        if  (i + 1) % 1000 == 0:\n",
    "            loss += [running_loss/1000]\n",
    "            running_loss = 0\n",
    "        optimizer.step(x, preds, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L2 Loss Function and Gradients\n",
    "\n",
    "def l2_loss(preds, targets):\n",
    "    return 1/2*np.sum(np.square(preds-targets))\n",
    "\n",
    "def l2_grad(inputs, preds, targets):\n",
    "    weights_grad = np.dot(inputs.T, preds-targets)/len(preds)\n",
    "    bias_grad = np.dot(np.ones(len(targets)).T, preds-targets)/len(preds) \n",
    "    return weights_grad, bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2eae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L1 Loss Function and Gradients\n",
    "\n",
    "def l1_loss(preds, targets):\n",
    "    return np.sum(np.abs(preds-targets))\n",
    "\n",
    "def l1_grad(inputs, preds, targets):\n",
    "    # Taken by piecewise derivative calculation\n",
    "    weights_grad = np.dot(inputs.T, np.sign(preds-targets))/len(preds)\n",
    "    bias_grad = np.dot(np.ones(len(targets)).T, np.sign(preds-targets))/len(preds)\n",
    "    return weights_grad, bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4858a2",
   "metadata": {},
   "source": [
    "## 3.7 Weight Decay (Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2517fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
