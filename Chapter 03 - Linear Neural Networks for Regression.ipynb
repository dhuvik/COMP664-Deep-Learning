{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc7730a",
   "metadata": {},
   "source": [
    "# Chapter 3: Linear Neural Networks for Regression\n",
    "\n",
    "### Dhuvi Karthikeyan\n",
    "\n",
    "##### 1/11/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b55935",
   "metadata": {},
   "source": [
    "Minimal notes just on the things I had forgotten/wish to review for retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43b952",
   "metadata": {},
   "source": [
    "## 3.1 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259f96f",
   "metadata": {},
   "source": [
    "Affine transformation that decomposes into a linear transformation by weighted sum fo input features and then translation by the addition of a bias term. \n",
    "\n",
    "Conditional mean: $E[Y|X=x]$ -> MLE Estimates\n",
    "\n",
    "$$ y = \\textbf{w}^Tx + b + \\epsilon ~ N(0, \\sigma^2)$$\n",
    "\n",
    "$$ P(y | x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{(-\\frac{1}{2\\sigma^2}(y - \\textbf{w}^Tx -b)^2)}$$ \n",
    "\n",
    "$$ P(y | X) = \\prod_{i=1}^np(y^{(i)}|x^{(i)}) $$\n",
    "\n",
    "$$ -logP(y|X) = \\sum_{i=1}^n \\frac{1}{2}log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}(y^{(i)} - \\textbf{w}^Tx^{(i)} - b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b621e",
   "metadata": {},
   "source": [
    "## 3.2 Object-Oriented Design for Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89723a21",
   "metadata": {},
   "source": [
    "### 3.2.1 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7ec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function for adding attributes to objects after instantiation\n",
    "\n",
    "def add_to_class(Class):\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d88c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:\n",
    "    def __init__(self):\n",
    "        self.instantiated = True\n",
    "        \n",
    "c = C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e06bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(C)                  # Limits the scope of adding attributes to just the class\n",
    "def add_name(self, name):\n",
    "    self.name = name\n",
    "    \n",
    "def add_directive(obj, directive): # Global scope to all objects [Not best practices]\n",
    "    obj.directive = directive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02ad3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.add_name('ClassC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f1478f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ClassC'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b08d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_directive(c, \"Exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39167296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exist'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.directive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b6078e6",
   "metadata": {},
   "source": [
    "## 3.4 Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01ebd9",
   "metadata": {},
   "source": [
    "This is the main problem from HW1 and is pulled from my implementation in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baf11f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoader Class\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Implements a Data Loading Class for passing mini-batches to model after\n",
    "    minor processing to ensure that matrix multiplication works. Assumes shuffle\n",
    "    to be true.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "          self, \n",
    "          inputs,\n",
    "          labels,\n",
    "          batch_size\n",
    "    ):\n",
    "        self.__len__ = len(labels)\n",
    "        self.data = self.gen_tensor(inputs)\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.called_idx = np.array([], dtype=int)\n",
    "        self.prefetch = self.__get_idx__()\n",
    "\n",
    "  \n",
    "    def reshape_input(self, inputs):\n",
    "        '''Reshapes input of dims >= 1 to a matrix of n x n_features'''\n",
    "        n_examples = inputs.shape[0] #Assumes first dim is n_examples\n",
    "        assert n_examples == self.__len__\n",
    "        return inputs.reshape(n_examples, int(inputs.size/n_examples))\n",
    "\n",
    "        def gen_tensor(self, inputs):\n",
    "            '''Checks for iterable inputs and runs reshape above''' \n",
    "            try:\n",
    "                iter(inputs)\n",
    "            except TypeError:\n",
    "                print(\"Inputs is not an iterable.\")\n",
    "            return self.reshape_input(inputs)\n",
    "\n",
    "    def __get_idx__(self):\n",
    "        '''Check that we have enough examples for another batch_size'''\n",
    "        if self.__len__ - len(self.called_idx) < self.batch_size:\n",
    "          # On epoch end reset the called indices\n",
    "          self.called_idx = np.array([], dtype=int)\n",
    "        remaining_idx = np.delete(np.arange(self.__len__), list(self.called_idx))\n",
    "        idx = np.random.choice(remaining_idx, self.batch_size, replace=False)\n",
    "        self.called_idx = np.append(self.called_idx, idx)\n",
    "        return idx\n",
    "\n",
    "    def _get_item_(self):\n",
    "        '''Get a batch of the data when called with batch_size'''\n",
    "        sampled_idx = self.__get_idx__()\n",
    "        return (self.data[sampled_idx, :], self.labels[sampled_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81041d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LinearRegression Model Class\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Implements a Linear Regression Model Class for performing linear regression\n",
    "    without the closed form solution. Randomly initializes weights and biases \n",
    "    before using mini-batch SGD to optimize weights\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self,\n",
    "      input_dim\n",
    "    ):\n",
    "        self.weights = np.random.randn(input_dim)\n",
    "        self.bias = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #Vectorized implementation of Forward Call w// broadcasting for bias\n",
    "        return X @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b084f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gradient Descent Class\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Implements gradient descent algorithm. Works with arbitrary models\n",
    "    and loss functions (whose gradients are manually implemented).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model, \n",
    "        learning_rate,\n",
    "        grad_fxn\n",
    "    ):\n",
    "        self.eta = learning_rate\n",
    "        self.grad = grad_fxn\n",
    "        self.model = model\n",
    "\n",
    "    def step(self, inputs, preds, labels):\n",
    "        grads = self.grad(inputs, preds, labels)\n",
    "        self.update_params(grads)\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        params = self.model.params.keys()\n",
    "        for i,p in enumerate(params):\n",
    "            self.model.params[p] -= self.eta*grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841df9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Train_Iters Function\n",
    "def train_iters(iterations, data_loader, model, lossfn, optimizer):\n",
    "    running_loss = 0\n",
    "    loss = []\n",
    "    for i in tqdm(range(iterations)):\n",
    "        x,y = data_loader._get_item_()\n",
    "        preds = model.forward(x)\n",
    "        running_loss += lossfn(preds, y)\n",
    "        if  (i + 1) % 1000 == 0:\n",
    "            loss += [running_loss/1000]\n",
    "            running_loss = 0\n",
    "        optimizer.step(x, preds, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L2 Loss Function and Gradients\n",
    "\n",
    "def l2_loss(preds, targets):\n",
    "    return 1/2*np.sum(np.square(preds-targets))\n",
    "\n",
    "def l2_grad(inputs, preds, targets):\n",
    "    weights_grad = np.dot(inputs.T, preds-targets)/len(preds)\n",
    "    bias_grad = np.dot(np.ones(len(targets)).T, preds-targets)/len(preds) \n",
    "    return weights_grad, bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2eae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L1 Loss Function and Gradients\n",
    "\n",
    "def l1_loss(preds, targets):\n",
    "    return np.sum(np.abs(preds-targets))\n",
    "\n",
    "def l1_grad(inputs, preds, targets):\n",
    "    # Taken by piecewise derivative calculation\n",
    "    weights_grad = np.dot(inputs.T, np.sign(preds-targets))/len(preds)\n",
    "    bias_grad = np.dot(np.ones(len(targets)).T, np.sign(preds-targets))/len(preds)\n",
    "    return weights_grad, bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f796629",
   "metadata": {},
   "source": [
    "## 3.6 Generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f8f09",
   "metadata": {},
   "source": [
    "### 3.6.1 Training Error and Generalization Error\n",
    "\n",
    "* Strong IID assumption for training data and test data\n",
    "* $R_{emp}$ is the empirical risk which is also known as the training error and is a statistic that is computed on the training data and also a function of the model\n",
    " $$ R_{emp}[X, y,f] = \\frac{1}{n}\\sum_{i\\in \\text{Data}}^nl(x_i, y_i, f(x_i))$$\n",
    " \n",
    " * The generalization error on the other hand is an expectation over all the data re: a certain models predictions and the true values:\n",
    " \n",
    " $$ R[p, f] = E_{(x,y)~P}[l(x, y, f(x)] = \\int \\int l(x, y, f(x))p(x,y)dxdy $$\n",
    " \n",
    " * Since we can never observe the population of data we instead holdout some of the training data which we take to be IID with the rest of the non-heldout training data and then use that as teh generalization set. \n",
    " \n",
    " * Pointing a fixed classifier on the test set to estimate the TRUE error is a problem of mean estimation. (How off is the sample mean from the population mean or true parameter)\n",
    " \n",
    " #### Model Complexity \n",
    " \n",
    " * When we start working with more complicated models, we actually expect the training loss and the validation looss to begin to diverge.\n",
    " \n",
    "     * The problem of overfitting the training data: with that number of free parameters it makes perfect sense that an overly expressive model would be able to predict the training labels even if the labels were generated at random\n",
    "     \n",
    " * Model complexity can describe both model architecture as well as the parameter ranges that the free params can take\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09f721",
   "metadata": {},
   "source": [
    "### 3.6.2 Underfitting or Overfitting?\n",
    "\n",
    "* Overfitting is when the training loss is close to zero and the validation loss is high and often can appear to be diverging. \n",
    "* Underfitting is when the training loss and validation loss are both high and there is barely any difference between the two\n",
    "\n",
    "#### Dataset Size\n",
    "\n",
    "* Probability of overfitting is inversely proportional to the dataset size\n",
    "* For a fixed task and data distribution, the model complexity increasing **shouldnt outpace** data generation capability.\n",
    "    * Deep learning can only outperform linear models given that there are many thousands of data examples to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e00105",
   "metadata": {},
   "source": [
    "### 3.6.3 Model Selection\n",
    "\n",
    "* Test set shouldn't be touched until the hyperparameters and model features are chosen beforehand\n",
    "* Three way split to the data:\n",
    "    * Training Data: Useful for figuring out which hyper-params to use\n",
    "    * Test Data: This is during inference time\n",
    "    * Validation Data: This can be used for model gets the lowest validation error\n",
    "\n",
    "#### Cross-Validation \n",
    "\n",
    "* K-fold cross validation is a strategy that is used when there is a sparse amount of training data such that the experimentalist cannot afford to sequester data for validation. \n",
    "    * The data needs to be split into K non-overlapping subsets and model training + validation is used on the data K times where each time 1 of the subsets is used as validation and K-1 is used as the training data.\n",
    "    * Average over the k-runs of doing this to figure out the overall training error and the test error.\n",
    "        * Is there any way to even go over the different dynamics of the loss curves over the different validations?\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4858a2",
   "metadata": {},
   "source": [
    "## 3.7 Weight Decay (Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609abbd",
   "metadata": {},
   "source": [
    "### 3.7.1 Norms and Weight Decay\n",
    "\n",
    "* Instead of varying the number of parameters, we instead vary the range of values it can take (dampening it) which is essentially the same as feature selection because setting these weights to zero is essentially the same thing.\n",
    "\n",
    "* Weight decay is equivalent to be the $l-2$ regularization for stochastic gradient descent but maybe not for other optimization algorithms\n",
    "     * Might be the most widely used regularization method for parameteric models\n",
    "     \n",
    " * Adding the norm of the weight vector to the loss function as a penalty term penalizes large values of the weights\n",
    " \n",
    " $$ L(w,b) + \\frac{\\lambda}{2}||w||^2$$\n",
    " \n",
    " * The one half is there as a convenient feature for differentiation of the loss function and teh square is included as a means of getting rid of the square root introduced by the l-2 loss. This is just chosen because of simplicity because the L1 has the absolute value which is finicky to use.\n",
    " \n",
    "$$ w <- (1 - \\eta \\lambda)w - \\frac{\\eta}{|\\beta|}\\sum_{i \\in B}\\nabla f^{(i)}(x^{(i)}, y^{(i)})$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921c51c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "055715ea",
   "metadata": {},
   "source": [
    "### Dropout Notes (From Ch 5)\n",
    "\n",
    "* In regularizaiton we want to learn smooth functions such that the addition of arbirary noise to our data does not derail the outputs\n",
    "* 1995 - Christopher Bishop showed that training with input noise on purpose is equal to Tikonov regularization\n",
    "* 2014 - Dropout: a simple way to prevent neural networks from overfitting \n",
    "    * Zero out outputs at each layer and that way we \"noise\" the data during forward prop (How?)\n",
    "    * This allows for it to gradients to be applied in backprop to only a certain subset of the data\n",
    "* How is the noise injected?\n",
    "    * In the original version in 1995, Bishop stochastically added Gaussian noise to the input\n",
    "        * In expectation $\\mathbb{E}[h'] = h$\n",
    "    * In the 2014 dropout version they set the output of a node to 0 with probability p and renormalize the remaining outputs to h/1-p with probability 1-p so in expectation: $\\mathbb{E}[h']= p*0 + (1-p)*\\frac{h}{1-p} = h$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
