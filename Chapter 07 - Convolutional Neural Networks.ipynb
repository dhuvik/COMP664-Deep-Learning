{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d4e478",
   "metadata": {},
   "source": [
    "# Chapter 7: Convolutional Neural Networks\n",
    "\n",
    "### Dhuvi Karthikeyan\n",
    "\n",
    "2/07/2023\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81798ac7",
   "metadata": {},
   "source": [
    "## 7.1 From Fully Connected Layers to Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3230c7c",
   "metadata": {},
   "source": [
    "The assumptions of tabular data:\n",
    "    \n",
    "    * Tabular data is where the rows are examples and the columns are features\n",
    "    * Assume no a priori structure on how the features interact between each other\n",
    "    * A one megapixel image would need 10^6 [Input_Dim] * Hidden_Dim params for the first layer \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76711f04",
   "metadata": {},
   "source": [
    "### 7.1.1 Invariance\n",
    "\n",
    "Desiderata:\n",
    "\n",
    "* Early layers need to focus on more local features (locality) whereas later layers need to focus on global features\n",
    "* Early layers should respond similarly to similar patches of the image, regardless of where it appears in the global image (translational equivariance).\n",
    "    * Invariance vs. Equivaraince: \n",
    "        * Invariance: Variances are equal to 0\n",
    "        * Equivariance: The variances are equal and non-zero\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c80711",
   "metadata": {},
   "source": [
    "### 7.1.2 Constraining the MLP\n",
    "\n",
    "Define $\\textbf{X}$ to be the input image with immediate hidden representation tensor $ \\textbf{H} \\in \\mathbb{R}^2$ where they both have the same shape. To have all of the input pixels inform each hidden state index, we would need a 4th order weight tensor to index the $X_{i,j}$ and $H_{k,l}$ interaction. Each pixel in the input would need a bias so the bias vector would be a bias matrix $\\textbf{U}$.\n",
    "\n",
    "\n",
    "#### Formulation\n",
    "$$ \\textbf{H}_{ij} = \\textbf{U}_{i,j} + \\sum_{k}\\sum_{l}W_{i,j,k,l}X_{k,l}$$\n",
    "$$ = \\textbf{U}_{i,j} + < W_{i,j}, X>_F $$\n",
    "\n",
    "This is essentially indexing the fourth order weights tensor along the first two axes for the resulting matrix that corresponds to the i,j index of the hidden input. For each $\\textbf{H}_{i,j}$ multiply each entry of matrix $W_{ij}$ and entry of $X$ together, taking the product summing over all of these, which is the Frobernius inner product of two matrices $W_{ij}$ $X$. To this inner product we add noise from the noise matrix, specifically the entry at (i,j).\n",
    "\n",
    "#### Translation Equivariance\n",
    "\n",
    "For translation equivariance/invariance we need the hidden representation of i,j to be agnostic to where we are in the hidden dim, and take into account only where we are in the image. As such we remove the (i,j) dependence from the bias matrix U and instead have a constant u. We also reduce the 4th order tensor to a 2nd order one (matrix), and get the following:\n",
    "\n",
    "$$ \\textbf{H}_{i,j} = u + \\sum_k \\sum_l \\textbf{W}_{k,l}X_{k,l}$$\n",
    "\n",
    "#### Locality\n",
    "\n",
    "The idea behind this principle is to zero-out the pixels outside a specific window of the target pixel such that we focus on the local features of the image and incorporate that into the hidden state at i,j. We start off the formulation by first re-indexing the pixels of the image such that we loop over a and b which run over positive and negative numbers such that the edge of the images are reached but never crossed.\n",
    "\n",
    "$$ \\text{Reindexing gives: } \\textbf{H}_{i,j} = u + \\sum_{a}\\sum_{b}W_{a,b}X_{i+a,j+b}$$ \n",
    "$$ \\textbf{H}_{i,j} = u + \\sum_{a=-\\Delta}^\\Delta\\sum_{b=-\\Delta}^\\Delta W_{a,b}X_{i+a,j+b}$$\n",
    "\n",
    "The equation above is also known as a convolutional layer. The weight matrix of a convolutional layer of dimensions $2\\Delta x 2\\Delta$ what is also known as the convolution kernel or a filter. We have thusly offloaded a number of the parameters by imposing an inductive bias, which when it agrees with the data, results in sample efficient models that generalize well.\n",
    "\n",
    "#### Desideratum #3\n",
    "The last thing we need is to ensure that the deeper layers of the models should respond to more higher level features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297463e",
   "metadata": {},
   "source": [
    "### 7.1.3 Convolutions\n",
    "\n",
    "In math a convolutioin between is an operation that takes two functions f and g as input and outputs the following:\n",
    "\n",
    "$$ (f*g)(x) = \\int f(z)g(x-z)dz$$\n",
    "\n",
    "Or in the discrete case:\n",
    "\n",
    "$$ (f*g)(i) = \\sum_a f(a)g(i-a)$$\n",
    "\n",
    "\n",
    "For the case of two-dimensional tensors:\n",
    "\n",
    "$$(f*g)(i,j) = \\sum_a \\sum_b f(a,b)g(i-a, j-b)$$\n",
    "\n",
    "which is almost like the $\\textbf{H}_{i,j}$ except for the divisions. Although we call it convolutions the actual procedure used for $\\textbf{H}_{i,j}$ is a cross-correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5273ef",
   "metadata": {},
   "source": [
    "### 7.1.4 Channels \n",
    "\n",
    "Since images consist of three channels, we make the following adjustments:\n",
    "\n",
    "* Add a dimension to the hidden representation and the convolutional kernel\n",
    "* Add a dimension to the bias term?\n",
    "* To add input channel to output channel flexibility s.t. each output channel gets weighted info from each input channel, we add a fourth dimension to the convolutional kernel.\n",
    "\n",
    "$$ \\textbf{H}_{i,j,d} = U_d + \\sum_{a=-\\Delta}^\\Delta\\sum_{b=-\\Delta}^\\Delta\\sum_c W_{a,b,c,d}X_{i+a,j+b, c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca02904",
   "metadata": {},
   "source": [
    "## 7.2 Convolutions for Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006789f7",
   "metadata": {},
   "source": [
    "### 7.2.1 Cross-Correlation Operation\n",
    "\n",
    "Convolutional layers are a misnomer since the actual operation is a cross-correlation. In 2-D cross-correlations the filter is placed on the corner and slid across the image left to right and top to bottom. At each point the kernel and the input tensor are processed via frobenius inner product and the output tensor is constructed elementwise. \n",
    "\n",
    "In this formulation the output tensor is **(image_height - kernel_height + 1) x (image_width - kernel_width + 1)** in dimension. In practice, this the output tensor retains the same dimensions as the input image by padding the input image with 0's at the borders to allow for the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b00eb9",
   "metadata": {},
   "source": [
    "### 7.2.2 Convolutional Layers\n",
    "\n",
    "Convolutional kernels are usually randomly initialized, the same as MLP weights. They are likewise updated by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013b446",
   "metadata": {},
   "source": [
    "### 7.2.3 Object Edge Detection in Images\n",
    "\n",
    "Using a trivial example, of an \"image\" with columns that are either all 1 or all 0 it can be shown that with a convolutional filter of [1, -1] that we can indeed detect edges in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8c017ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def corr2d(X, K):\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8f31174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a031d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0e190dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd123075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose doesn't work\n",
    "\n",
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34d1dafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-1., -1., -1., -1., -1., -1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposing the filter fixes this \n",
    "corr2d(X.t(), K.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a2392",
   "metadata": {},
   "source": [
    "### 7.2.4 Learning a Kernel\n",
    "\n",
    "By gradient descent after 10 iterations, a single convolutional layer is able to learn the parameters of the kernel for edge detection using input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e80c48",
   "metadata": {},
   "source": [
    "### 7.2.5 Cross-Correlation and Convolution\n",
    "\n",
    "Where $f(a,b) = F_{a,b}$, $g(a,b) = G_{a,b}$ for tensors F and G\n",
    "\n",
    "\n",
    "Strict convolution:\n",
    "\n",
    "$$(f*g)(i,j) = \\sum_a \\sum_b f(a,b)g(i-a, j-b)$$\n",
    "\n",
    "Cross-Correlation:\n",
    "\n",
    "$$(f*g)(i,j) = \\sum_a \\sum_b f(a,b)g(i+a, j+b)$$\n",
    "\n",
    "To get from cross correlation and convolution the only operation we change is what is happening to g, the filter. So by flipping the filter horizontally and vertically we invert the subtraction. Whether we use convolution and the flipped kernel or cross-correlation and the regular kernel, the same output will be generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c967f9",
   "metadata": {},
   "source": [
    "### 7.2.6 Feature Map and Receptive Field\n",
    "\n",
    "**Feature Map**: The output of a convolutional layer is also called a feature map since the learned features (edge detection among other things) from the previous layer and passes these as learned representations to coming layers. \n",
    "\n",
    "**Receptive Field**: The receptive field of an element x in a layer is the set of all elements from previous layers that may affect the calculation of x during forward pass (can be larger than input size depending on layer depth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476060d7",
   "metadata": {},
   "source": [
    "## 7.3 Padding and Stride\n",
    "\n",
    "Padding allows one to retain input dimension whereas stride allows one to drastically reduce it. Padding also importantly ensures that the frequency of pixel usage is invariant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0da5b",
   "metadata": {},
   "source": [
    "### 7.3.1 Padding\n",
    "\n",
    "To increase the image output dimensions by padding_height and padding_width:\n",
    "**(image_height - kernel_height + padding_height + 1) x (image_width - kernel_width + padding_width + 1)**\n",
    "\n",
    "Most cases, padding_height and width will be set to kernel_height and width -1. There is a preference for odd dimensions for the kernel such that we can evenly distribute the padding and also the interpretation is smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aeeb1f",
   "metadata": {},
   "source": [
    "### 7.3.2 Stride\n",
    "\n",
    "Moving the convolutional kernel more than one pixel at a time has benefits for downsampling the input representation (noising) as well as computational benefit. In the two dimensional example, the stride is for both rows and columns traversed per step. \n",
    "\n",
    "**Stride Formula:**\n",
    "\n",
    "$$ \\lfloor (image_height - kernel_height + padding_height + stride_height)/stride_height \\rfloor x \\lfloor(image_width - kernel_width + padding_width + stride_width)/stride_width \\rfloor$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf76535",
   "metadata": {},
   "source": [
    "## 7.4 Multiple Input and Multiple Output Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27347b",
   "metadata": {},
   "source": [
    "### 7.4.1 Multiple Input Channels\n",
    "\n",
    "With multiple channels there is barely a difference. We simply must ensure that the kernel is of the same number of dimensions and then the cross-correlation operation is performed on the different channels by the different kernels. The resulting output split by channel is then summed elementwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9fbef",
   "metadata": {},
   "source": [
    "### 7.4.2 Multiple Output Channels\n",
    "\n",
    "Popular neural network architectures actually increase the number of channels with layer depth, trading off spatial resolution for channel depth.  This is because typically channels correspond to their own filters and each filter represents its own spatial feature (edge detections, circle detection, etc...). For each output channel we have input_channels * kernel_height * kernel_width tensor concatenated along the 0th axis for dimension output_channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8cd4a2",
   "metadata": {},
   "source": [
    "### 7.4.3 1x1 Convolutional Filter\n",
    "\n",
    "The use case for a 1x1 convolutional filter is skeptical because the cross-correlation function is less intuitive and arbitrary in this case, but this architecture choice is included in a few DL approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a255514",
   "metadata": {},
   "source": [
    "### 7.4.4 Discussion\n",
    "\n",
    "Calculation of kxk convolution in an image of (hxw) is O(h*w*k*k). The inclusion of input and output channels make this an O(h*w*k*k*i*o) operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533555c",
   "metadata": {},
   "source": [
    "## 7.5 Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744fb13",
   "metadata": {},
   "source": [
    "### 7.5.1 Maximum Pooling and Average Pooling\n",
    "\n",
    "Pooling operators like convolutional kernels are filters applied to regions of the input according to stride and compute a scalar output for each step. This pooling window computes a deterministic operation (min, max, avg). Max-pooling is almost always preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08e6c5",
   "metadata": {},
   "source": [
    "### 7.5.2 Padding and Stride\n",
    "\n",
    "Since pooling operations change the output shape, padding and stride can be played with to get the output the desired shape.\n",
    "\n",
    "\n",
    "Padding and stride are usually kept identical so a maxpool(2) is a downsample such 1/2 of the length and width are taken resulting in a 1/4 downsampling. MaxPool(3) is a 1/9th downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c3a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
