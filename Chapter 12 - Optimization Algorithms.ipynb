{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811538d",
   "metadata": {},
   "source": [
    "# Chapter 12: Optimization Algorithms\n",
    "\n",
    "\n",
    "### Dhuvi Karthikeyan\n",
    "\n",
    "##### 1/22/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8fc9b",
   "metadata": {},
   "source": [
    "## 12.1 Optimization and Deep Learning\n",
    "\n",
    "Even though most of deep learning (if not all) deals with optimizing paramater spaces that are non-convex with respect to their losses, lessons learned from convex optimization serve as guideposts in non-convex optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb2230",
   "metadata": {},
   "source": [
    "### 12.1.1 Goal of Optimization\n",
    "\n",
    "The goal of optimization in deep learning is to minimize the empirical risk which is the cost function over the training data (since we cannot see the test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f317cde",
   "metadata": {},
   "source": [
    "### 12.1.2 Optimization Challenges in Deep Learning\n",
    "\n",
    "#### Local Minima\n",
    "\n",
    "One of the benefits of mini-batch stochastic gradient descent is the ability to nudge the parameters out of a minima by way of randomizing the inputs that are being iterated on.\n",
    "\n",
    "#### Saddle Points\n",
    "\n",
    "More common than local minima and local maxima due to the nature in which they arise\n",
    "\n",
    "##### EIgenvalues of the Hessian Matrix\n",
    "\n",
    "Asssume that the input of the loss function is a k-dimensional vector and it outputs a scalar (as most loss functions do). The resulting Hessian matrix of such a function will contain k-eigenvalues.\n",
    "\n",
    "* When the eigenvalues at the position where the gradient=0 are all positive, that position is a local minimum\n",
    "* When the eignevalues @grad=0 are all negative, that position is a local maximum.\n",
    "* When the eigenvalues @grad=0 are mixed positive and negative , that position is a saddle point.\n",
    "\n",
    "#### Vanishing Gradients\n",
    "\n",
    "Most insidious problem. Happen quite often. Requires interesting trick. Alleviated partially by switch from the tan function to the ReLU activation function. Other reparameterization tricks have been useful in this space as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fddd729",
   "metadata": {},
   "source": [
    "## 12.2 Convexity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6235f0f",
   "metadata": {},
   "source": [
    "### 12.2.1 Definitions\n",
    "\n",
    "#### Convex Sets\n",
    "\n",
    "A convex set in a vector space is one where for every pair of points in the set there is a line that connects them, where every point of the line is also contained in the set. \n",
    "\n",
    "* The intersection of two convex sets is itself convex\n",
    "* Most deep learning problems occur in convex sets. For example variables in $\\mathbb{R}$, any lines between two points in that space stay in $\\mathbb{R}$. \n",
    "\n",
    "#### Convex Functions\n",
    "\n",
    "Given a convex set $X$, a convex function is one that retains the following properties given any points $a,b \\in X$ and $\\lambda \\in [0,1]$:\n",
    "\n",
    "$$ \\lambda f(a) + (1-\\lambda) f(b) >= f(\\lambda a + (1-\\lambda)b) $$\n",
    "\n",
    "* Some functions can be locally complex over a defined period whereas not be convex overall.\n",
    "\n",
    "\n",
    "#### Jensen's Inequality\n",
    "\n",
    "This is a generalization of the definition of the convex function. Instead of lambda we have $\\alpha_i \\in \\alpha \\text{ s.t.} \\sum_i \\alpha_i = 1$\n",
    "\n",
    "$$ \\sum_i \\alpha_i f(x_i) >= f(\\sum_i \\alpha_i x_i)$$\n",
    "\n",
    "This is basically saying that the convex function of an expectation is no greater than the expectation of a convex function (if we take $\\alpha_i$ to be a probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37908a8a",
   "metadata": {},
   "source": [
    "### 12.2.2 Properties\n",
    "\n",
    "#### Local Minima are Global Minima\n",
    "\n",
    "This is easily shown via Jensen's inequality and the definition of a convex function. \n",
    "\n",
    "#### Convexity and Second Derivative\n",
    "\n",
    "A function of a single variable is convex if and only if its second derivative >= 0. A multivariable function is convex iff its Hessian matrix is positive semi-definite meaning $x^T H x >= 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f3911",
   "metadata": {},
   "source": [
    "### 12.2.3 Constrains\n",
    "\n",
    "Convex optimization allows for the (easier) handling of optimization with constraints, aka constrained optimization. \n",
    "\n",
    "$$ \\mathtt{minimize}_x f(x) $$\n",
    "$$ \\text{s.t. }c_i(s) <= 0 \\text{ for all i $\\in$ \\{1, ..., n\\}}$$\n",
    "\n",
    "#### Lagrangian\n",
    "\n",
    "A method of repackaging a constrained optimization problem as an unconstrained optimization problem using linear algebra and multivariable calculus. \n",
    "\n",
    "$$ L(\\mathbf{x}, \\alpha_1, ..., \\alpha_n) = f(\\mathbf{x}) + \\sum_{i=1}^n \\alpha_i c_i \\mathbf{x} $$\n",
    "\n",
    "$\\alpha_i$ are the Langrangian multipliers and they reflect the rate of change of the value trying to be optimized with the value that the constraint function equals. This formulation is easy to implement in computers because we are in essence finding the gradient of the Lagrangian and setting it equal to zero which directly codes a number of dependencies between the cost function and the function we want to optimize f(x). In the above formulation, which is a saddle point optimization problem, one would want to maximize L w.r.t $\\alpha_i$ and minimize it w.r.t $\\mathbf{x}$. \n",
    "\n",
    "#### Penalties\n",
    "\n",
    "Approximate satisfaction of the lagrangian can be done by instsead of coercing $c_i(\\textbf{x})$ to be equal to or less than 0, we instead add $\\alpha_i c_c(\\mathbf{x})$ to the function $f(x)$. **This is actually often more robust than the exact method especially in non-convex optimization** due to assumptions of optimality that are broken when convex assumptions are relaxed.\n",
    "\n",
    "#### Projections\n",
    "\n",
    "Projections can also be used to satisfy constraints. Not super touched on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746acc5c",
   "metadata": {},
   "source": [
    "## 12.3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a0fc4",
   "metadata": {},
   "source": [
    "### 12.3.1 Gradient Descent in 1-Dimension\n",
    "\n",
    "$$ f(x + e) = \\sum_{n=0}^\\infty \\frac{e^nf^{(n)(x)}}{n!}$$\n",
    "\n",
    "One of the methods of motivating gradient descent comes from Taylor expansion of the loss function. Take the loss function $f(x)$ and apply the Taylor expansion about the point $\\epsilon$, approximating it to the first-order\n",
    "\n",
    "$$ f(x) \\approx f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)}{2!}(x-x_0)^2 + ... $$\n",
    "\n",
    "Setting $x-x_0 = \\epsilon$ and substituting $x = x_0 + \\epsilon$: \n",
    "\n",
    "$$ f(x_0 + \\epsilon) = f(x_0) + \\epsilon f'(x_0) + \\mathbb{O}(\\epsilon^2) $$\n",
    "\n",
    "Now if we want to minimize f(x) we can choose epsilon such that $f(x_0 + \\epsilon)$ gets iteratively lesser. By choosing $\\epsilon = -f'(x)$ we can ensure that the x we are at moves in the direction of a decreasing gradient. Because if the gradient is positive then we subtract that value from x and move in the reverse direction going back down the curve and if the gradient is negative at that point we subtract a negative gradient and move forward along the x axis pushing us down into the curve. We also select a hyperparameter $\\eta$ which we multiply by the gradient which controls how far we step in any direction:\n",
    "\n",
    "$$ f(x-\\eta f'(x)) = f(x) = \\eta f^{'2}(x) + \\mathbb{O}(\\eta^2f^{'2}(x)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a2619a",
   "metadata": {},
   "source": [
    "### 12.3.2 Multivariate Gradient Descent\n",
    "\n",
    "We can extend the derivation from the single variable into the multivariable stage to get:\n",
    "\n",
    "$$ f(\\mathbf{x} + \\mathbf{\\epsilon}) = f(\\mathbf{x}) + \\epsilon^{\\mathtt{T}}\\nabla f(\\mathbf{x}) + \\mathbb{O}(||\\mathbf{\\epsilon}||^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12234b",
   "metadata": {},
   "source": [
    "### 12.3.3 Adaptive Methods\n",
    "\n",
    "Choosing the perfect value of the learning rate can be rather tricky and even when chose well (finding one that causes training to converge) it can be slow. Tuning the learning rate depending on where we are in the curve seems like a desirable feature. We can do so by leveraging what we know about the second derivative. These methods aren't directly applicable to deep learning methods due to their non-convex form and the cost of computation to find these non-convex optimizers but builds intuition for clever tricks to be used in nonconvex setting.\n",
    "\n",
    "#### Netwon's Method\n",
    "\n",
    "\n",
    "$$ f(\\mathbf{x} + \\mathbf{\\epsilon}) = f(\\mathbf{x}) + \\epsilon^{\\mathtt{T}}\\nabla f(\\mathbf{x}) + \\frac{1}{2!}\\epsilon^{\\mathtt{T}}\\nabla^2 f(\\mathbf{x})\\mathbf{\\epsilon} + \\mathbb{O}(||\\mathbf{\\epsilon}||^3) $$\n",
    "\n",
    "The hessian matrix is implicitly included in the above as: $\\nabla^2 f(\\mathbf{x})$\n",
    "\n",
    "By taking the gradient with respect to $\\epsilon$ and ignoring HoTs we instead have a closed form solution for calculating the optimal learned rate parameter which is equal to:\n",
    "\n",
    "$$ \\nabla f(\\mathbf{x}) + \\mathbf{H\\epsilon} = 0 \\text{  thus, } \\mathbf{\\epsilon} = -\\mathbf{H}^{-1}\\nabla f(\\mathbf{x})$$\n",
    "\n",
    "This approach breaks when Hessian is negative so absolute values are sometimes necessary but can also be avoided by tuning learning rate. If Hessian is way too large, computing the hessian becomes infeasible and then inverting it becomes impossilbe.\n",
    "\n",
    "#### Convergence Analysis\n",
    "\n",
    "Arguably not super useful as it is only for a special constrcution of the problem.\n",
    "\n",
    "Assumptions:\n",
    "* The cost function is thrice-differentiable (f'' >0)\n",
    "* Function is convex\n",
    "\n",
    "We have quadratically decreasing error which means linear convergence and linearly decreasing error means constant convergence.\n",
    "\n",
    "#### Preconditioning\n",
    "\n",
    "$$ \\mathbf{x} <- \\mathbf{x} + \\eta \\mathtt{diag}(\\mathbf{H})^{-1}f(\\mathbf{x})$$\n",
    "\n",
    "Taking the diagonal entries of the hessian alleviates issues of comptuational cost of working with the entire Hessian. It effectively equals using a different learning rate on the different variables $x_i$ which is useful for features that have varying scales.\n",
    "\n",
    "* Also super useful for when the loss function with respect to each parameter is very different\n",
    "* The inversion step here is critical because if we have a high concavity we want to take a small step\n",
    "    * If we have a flat area we can affort to take a large step\n",
    "\n",
    "#### Gradient Descent with Line Search\n",
    "\n",
    "Line search was designed to address two central concerns in gradient descent based optimization methods: overshooting the target and making insufficient progress. \n",
    "\n",
    "Line search works by identifying the direction of gradient and then searching log-space for a learning rate that maximizes the descent. It converges rapidly but is less feasible for deep learning as it requries a large computational overhead to process the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897c52c",
   "metadata": {},
   "source": [
    "## 12.4 Stochastic Gradient Descent\n",
    "\n",
    "$$ \\mathbb{E}_i[\\nabla_\\theta Li (\\theta)] = \\frac{1}{n} \\sum_i^n\\nabla_{\\theta}L_i(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00154f5b",
   "metadata": {},
   "source": [
    "### 12.4.1 Stochastic Gradient Updates\n",
    "\n",
    "The computational cost of full gradient descent is $\\mathbb{O}(n)$ because the objective function is applied to every single datapoint before the average is taken for that epoch. Thus the larger the dataset gets, the more computational cost. \n",
    "\n",
    "In stochastic gradient descent we sample one random index at uniform and perform gradient descent on that $\\mathbb{O}(1)$. By virtue of the mean being an unbiased estimator of the population, we see that SGD is an unbiased estimator of GD. \n",
    "\n",
    "\n",
    "Sampling without replacement is preferred for a reason explained in 12.4.4\n",
    "\n",
    "\n",
    "The injecting of stochasticity by instantaneous gradient at $\\eta \\nabla f_i (\\mathbf{x})$ means that we may not ever converge at a minimum. This is where a dynamic learning comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e1d16",
   "metadata": {},
   "source": [
    "### 12.4.2 Dynamic Learning Rate\n",
    "\n",
    "* Piecewise constant decay: Decrease learning rate whenever learning stalls\n",
    "$$ \\eta (t) = \\eta_i if t_i <= t <= t_{i+1} $$\n",
    "* Exponential decay: decrease the learning rate exponentially over learning epochs\n",
    "$$ \\eta (t) = \\eta_0 * \\exp(-\\lambda t)$$\n",
    "* Polynomial decay: A happy intermediate that has been shown to work well in practice with $\\alpha = 0.5$\n",
    "$$ \\eta (t) = \\eta_0 * (\\beta t + 1)^{- \\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558bb22",
   "metadata": {},
   "source": [
    "### 12.4.3 Convergence Analysis for Convex Objectives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d0bf8",
   "metadata": {},
   "source": [
    "### 12.4.4 Stochastic Gradient Descent and Finite Samples\n",
    "\n",
    "Sampling with replacement:\n",
    "\n",
    "P(choosing the data point at least once):\n",
    "\n",
    "1 - P(not choosing it ever) = $1 - (1-\\frac{1}{n})^n \\approx 1 - \\exp(-1) \\approx 0.63$\n",
    "\n",
    "P(chossing the data point exactly once):\n",
    "\n",
    "$n\\choose1$ $\\frac{1}{n}(1-\\frac{1}{n})^{n-1} \\approx \\exp(-1) \\approx 0.37$\n",
    "\n",
    "Sampling with replacement thus leads to a lot of data inefficiency so sampling without replacement is actually preferred. Data will thus be passed in random orders to ensure that the models do not have cyclical passes through the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4c44c",
   "metadata": {},
   "source": [
    "## 12.5 Minibatch Stochastic Gradient Descent\n",
    "\n",
    "Gradient descent f.k.a. batch gradient descent can be computationally intractable since loading such a large matrix to memory cnan be infeasible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4706df4c",
   "metadata": {},
   "source": [
    "### 12.5.1 Vectorization and Caches\n",
    "\n",
    "* On single CPU/GPU devices the multiple computational units, each with different memories and bandwiths can create bottlenecks in not only the computation of data but also the shuttling of data to and from memory.\n",
    "\n",
    "    * With modern computational power it usually the memory accesses that are the issue not the computational units\n",
    "    \n",
    "* To calculate the number of bytes the CPU can process: $N_{GHz} * 10^9 * N_{Cores} * 32 = X$ bytes per second\n",
    "\n",
    "    * 32 could be the number of registers?\n",
    "    \n",
    "* Initial access incurs significant overhead but due to burst read subsequent accesses are not too burdensome\n",
    "\n",
    "* Cache hierarchies that go from:\n",
    "    * Immediate Use For Computation (registry)\n",
    "        * Within chip queued for computation (L1-cache)\n",
    "            * Outside chip (l2 -cache)\n",
    "                * Batched from main memory\n",
    "                    * main memory\n",
    "\n",
    "are a compromise that helps move the information closer to processing and tries to frontload accesses during computation. This is the main motivation behind batches in deep learning.\n",
    "\n",
    "##### Example: Matrix Multiplication of 2 Arbitrary NxN matrices\n",
    "\n",
    "1. $A_{ij} = B_{i,:}C_{:,j}$ Element-wise dot products\n",
    "2. $A_{:,j} = BC_{:,j}$ Computing column-wise\n",
    "3. $A = BC$ Whole computation\n",
    "4. Batching B and C into smaller blocks and then computing those to find A one block at a time\n",
    "\n",
    "If $B \\in \\mathbb{R}^{mxn}$ and $C \\in \\mathbb{R}^{nxp}$ it takes $\\approx 2mnp$ floating point operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc958ff",
   "metadata": {},
   "source": [
    "### 12.5.2 Minibatches\n",
    "\n",
    "The standard deviation decreases by a factor of $b^{-1/2}$ for size batch size b. However the reduction in SD is counterweighed by increase in computational cost. Thus the batch size we would prefer would be the largest bsize we can fit onto memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c69626",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Batch gradient descent can make it look like no training is happening. Stochastic gradient descent gets noisy in the tail of training but can show a steep dropoff in loss. Mini-batch gradient descent shows the loss is less steep but also a flatter tail after training.\n",
    "\n",
    "### Note:\n",
    "\n",
    "The time to reach a specific loss is the highest for SGD, lower for smaller batch sizes, and even lower for greater batch sizes. Batch SGD shows comparable time performance to mini-batch SGD with a sufficient batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09da0e",
   "metadata": {},
   "source": [
    "## 12.6 Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd21d6",
   "metadata": {},
   "source": [
    "### 12.6.1 Basics\n",
    "\n",
    "#### The Concept of Leaky Averages\n",
    "\n",
    "What if there was a way to decrease the variance even further that can be done by mini-batch stochastic gradient descent? Increase the batch size or **take into account the previous gradients**\n",
    "\n",
    "$$ \\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1} $$\n",
    "\n",
    "This simple idea spawned the branch of methods called accelerated gradient methods of which momentum is probably the most widely known.\n",
    "\n",
    "https://distill.pub/2017/momentum/\n",
    "\n",
    "#### The Momentum Method\n",
    "\n",
    "$$ \\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1} $$\n",
    "$$ \\mathbf{x}_t = \\mathbf{x}_{t-1} + \\eta_t\\mathbf{v}_{t} $$\n",
    "\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* Momentum actually speeds up convergence beceause not only does it dampen, it also accelerates\n",
    "* Desirable for both batch gradient descent and stochastic gradient descent\n",
    "* Effective number of gradients is given by \n",
    "$$ \\frac{1}{1-\\beta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39783151",
   "metadata": {},
   "source": [
    "## 12.10 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785846e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
